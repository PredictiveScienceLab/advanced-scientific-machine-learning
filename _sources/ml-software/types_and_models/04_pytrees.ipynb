{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytrees to represent model parameters\n",
    "\n",
    "Pytrees is `Jax`'s solution to the problem of working with nested data structures. This is immensely useful when working with parameters of complex neural networks.\n",
    "You can read about pytrees [here](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "The definition is:\n",
    "\n",
    "> In JAX, we use the term pytree to refer to a tree-like structure built out of container-like Python objects. Classes are considered container-like if they are in the pytree registry, which by default includes lists, tuples, and dicts. That is: any object whose type is not in the pytree container registry is considered a leaf pytree; any object whose type is in the pytree container registry, and which contains pytrees, is considered a pytree.\n",
    "\n",
    "Let's see some examples.\n",
    "\n",
    "Some trivial pytrees are primitives, like `int`, `float`, `bool`.\n",
    "Also, `Jax` arrays are pytrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as tree_util\n",
    "\n",
    "x = jnp.array([1, 2, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef(*)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_structure(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples of arbitrary objects are also pytrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef((*, *, *))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = (1, x, 'hello')\n",
    "tree_util.tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef([*, *, *])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = [1, x, 'hello']\n",
    "tree_util.tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef({'a': *, 'b': *, 'c': *})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = {'a': 1, 'b': x, 'c': 'hello'}\n",
    "\n",
    "tree_util.tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it gets interesting. You can nest pytrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = {'tree1': {'a': 1, 'b': x, 'c': 'hello'},\n",
    "        'tree2': {'a': 1, 'b': (x, x, x), 'c': 'hello'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree1': {'a': 1, 'b': Array([1, 2, 3], dtype=int32), 'c': 'hello'},\n",
       " 'tree2': {'a': 1,\n",
       "  'b': (Array([1, 2, 3], dtype=int32),\n",
       "   Array([1, 2, 3], dtype=int32),\n",
       "   Array([1, 2, 3], dtype=int32)),\n",
       "  'c': 'hello'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef({'tree1': {'a': *, 'b': *, 'c': *}, 'tree2': {'a': *, 'b': (*, *, *), 'c': *}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaves of the pytree are the primitives, the arrays, and the tuples of primitives and arrays.\n",
    "They are shown by `*` above.\n",
    "You can get the leaves of a pytree as a flattened list with `jax.tree_leaves`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " 'hello',\n",
       " 1,\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " 'hello']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_leaves(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also flatten the tree with `jax.tree_flatten`.\n",
    "It returns a tuple of the leaves and a function that can reconstruct the tree from the leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " 'hello',\n",
       " 1,\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " Array([1, 2, 3], dtype=int32),\n",
       " 'hello']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_values, tree_type = tree_util.tree_flatten(tree)\n",
    "flat_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef({'tree1': {'a': *, 'b': *, 'c': *}, 'tree2': {'a': *, 'b': (*, *, *), 'c': *}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a flattened tree, you can put it back together with `jax.tree_unflatten`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree1': {'a': 1, 'b': Array([1, 2, 3], dtype=int32), 'c': 'hello'},\n",
       " 'tree2': {'a': 1,\n",
       "  'b': (Array([1, 2, 3], dtype=int32),\n",
       "   Array([1, 2, 3], dtype=int32),\n",
       "   Array([1, 2, 3], dtype=int32)),\n",
       "  'c': 'hello'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_unflatten(tree_type, flat_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Neural network parameters\n",
    "\n",
    "The most useful type of pytree for us is the one that contains `Jax` arrays.\n",
    "This is the structure that we will use to represent the parameters of our neural networks.\n",
    "Let's make a simple neural network by hand.\n",
    "We will use a dictionary to represent the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "@partial(vmap, in_axes=(0, None))\n",
    "def simple_nn(x, params):\n",
    "    W1 = params[\"layer1\"][\"W\"]\n",
    "    b1 = params[\"layer1\"][\"b\"]\n",
    "    W2 = params[\"layer2\"][\"W\"]\n",
    "    b2 = params[\"layer2\"][\"b\"]\n",
    "    return W2 @ jnp.tanh(W1 @ x + b1) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': {'W': Array([[-0.11168969,  0.58439565,  1.437887  ],\n",
       "         [ 0.533231  , -1.0117726 , -2.316002  ]], dtype=float32),\n",
       "  'b': Array([-1.5917008, -0.9385306], dtype=float32)},\n",
       " 'layer2': {'W': Array([[ 0.43686673, -0.5115205 ]], dtype=float32),\n",
       "  'b': Array([0.6714109], dtype=float32)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random as random\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "keys = random.split(key, 4)\n",
    "params = {\n",
    "    \"layer1\": {\n",
    "        \"W\": random.normal(keys[0], (2, 3)),\n",
    "        \"b\": random.normal(keys[1], (2,)),\n",
    "    },\n",
    "    \"layer2\": {\n",
    "        \"W\": random.normal(keys[2], (1, 2)),\n",
    "        \"b\": random.normal(keys[3], (1,)),\n",
    "    },\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how it works on a bunch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.6350196 ],\n",
       "       [ 0.3767303 ],\n",
       "       [ 0.7132992 ],\n",
       "       [ 0.88301647],\n",
       "       [-0.27681673],\n",
       "       [-0.17706287],\n",
       "       [-0.22115844],\n",
       "       [ 0.31149212],\n",
       "       [ 1.293994  ],\n",
       "       [ 0.97846043]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, subkey = random.split(keys[0])\n",
    "xs = random.normal(subkey, (10, 3))\n",
    "\n",
    "simple_nn(xs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add some fake data and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "ys = random.normal(subkey, (10,))\n",
    "\n",
    "def loss(params, xs, ys):\n",
    "    pred = simple_nn(xs, params)\n",
    "    return jnp.mean((pred - ys)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.0290194, dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(params, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the gradient of the loss function with respect to the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit\n",
    "\n",
    "grad_loss = jit(grad(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': {'W': Array([[-0.11328071, -0.06153299,  0.23609458],\n",
       "         [ 0.04657721, -0.01834877,  0.02984624]], dtype=float32),\n",
       "  'b': Array([ 0.2492916 , -0.12900007], dtype=float32)},\n",
       " 'layer2': {'W': Array([[-0.47350553, -1.016762  ]], dtype=float32),\n",
       "  'b': Array([1.0130714], dtype=float32)}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = grad_loss(params, xs, ys)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this. The parameters are a pytree, so the gradient is a pytree too.\n",
    "The structure of the pytree is the same as the structure of the parameters.\n",
    "But the leaves of the pytree are the gradients of the loss function with respect to the parameters.\n",
    "Great!\n",
    "This generalizes to any pytree, not just dictionaries.\n",
    "\n",
    "What do we do with this?\n",
    "Well, we can do gradient descent.\n",
    "We have to subtract a small multiple of the gradient from the parameters.\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': {'W': Array([[-0.10036162,  0.59054893,  1.4142776 ],\n",
       "         [ 0.5285733 , -1.0099378 , -2.3189864 ]], dtype=float32),\n",
       "  'b': Array([-1.61663  , -0.9256306], dtype=float32)},\n",
       " 'layer2': {'W': Array([[ 0.4842173, -0.4098443]], dtype=float32),\n",
       "  'b': Array([0.57010376], dtype=float32)}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params = tree_util.tree_map(\n",
    "    lambda x, g: x - 0.1 * g,\n",
    "    params, g\n",
    ")\n",
    "new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on here?\n",
    "The function `tree_map` applies a function to every leaf of a pytree.\n",
    "In this case, we are subtracting a small multiple of the gradient from every leaf of the pytree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that we wanted to add an L2 regularization term to the loss function.\n",
    "This means that we have to add the square of every parameter to the loss function.\n",
    "How do we square all parameters?\n",
    "We can use `tree_map` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': {'W': Array([[0.01247459, 0.34151828, 2.067519  ],\n",
       "         [0.28433532, 1.0236839 , 5.363865  ]], dtype=float32),\n",
       "  'b': Array([2.5335114, 0.8808397], dtype=float32)},\n",
       " 'layer2': {'W': Array([[0.19085254, 0.2616532 ]], dtype=float32),\n",
       "  'b': Array([0.4507926], dtype=float32)}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_2 = tree_util.tree_map(\n",
    "    lambda x: x ** 2,\n",
    "    params\n",
    ")\n",
    "params_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can just sum the squares using `tree_reduce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(13.411046, dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_reduce(\n",
    "    lambda x, y: jnp.sum(x) + jnp.sum(y),\n",
    "    params_2,\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, xs, ys):\n",
    "    pred = simple_nn(xs, params)\n",
    "    squared_error = jnp.mean((pred - ys)**2)\n",
    "    l2_norm = tree_util.tree_reduce(\n",
    "        lambda x, y: jnp.sum(x) + jnp.sum(y),\n",
    "        tree_util.tree_map(lambda x: x ** 2, params),\n",
    "        0.0\n",
    "    )\n",
    "    return squared_error + 0.1 * l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me introduce another useful function, `value_and_grad`.\n",
    "It returns the value of a function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "loss_and_grad = jit(value_and_grad(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3701239, dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, g = loss_and_grad(params, xs, ys)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': {'W': Array([[-0.13561864,  0.05534614,  0.523672  ],\n",
       "         [ 0.15322341, -0.2207033 , -0.43335414]], dtype=float32),\n",
       "  'b': Array([-0.06904855, -0.31670618], dtype=float32)},\n",
       " 'layer2': {'W': Array([[-0.38613218, -1.1190661 ]], dtype=float32),\n",
       "  'b': Array([1.1473536], dtype=float32)}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tuples\n",
    "\n",
    "[Named tuples](https://docs.python.org/3/library/collections.html#collections.namedtuple) are a useful way to represent data.\n",
    "It allows you to access the elements of a tuple by name, like a dictionary, but with the dot syntax.\n",
    "This is useful when you have a bunch of data that you want to pass around as a single object.\n",
    "Named tuples are also pytrees.\n",
    "\n",
    "You can make a named tuple like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNParameters(layer1=LayerParameters(W=Array([[-0.11168969,  0.58439565,  1.437887  ],\n",
       "       [ 0.533231  , -1.0117726 , -2.316002  ]], dtype=float32), b=Array([-1.5917008, -0.9385306], dtype=float32)), layer2=LayerParameters(W=Array([[ 0.43686673, -0.5115205 ]], dtype=float32), b=Array([0.6714109], dtype=float32)))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "NNParameters = namedtuple(\"NNParameters\", [\"layer1\", \"layer2\"])\n",
    "LayerParameters = namedtuple(\"LayerParameters\", [\"W\", \"b\"])\n",
    "\n",
    "params = NNParameters(\n",
    "    LayerParameters(\n",
    "        W=random.normal(keys[0], (2, 3)),\n",
    "        b=random.normal(keys[1], (2,)),\n",
    "    ),\n",
    "    LayerParameters(\n",
    "        W=random.normal(keys[2], (1, 2)),\n",
    "        b=random.normal(keys[3], (1,)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access individual elements of the tuple by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerParameters(W=Array([[-0.11168969,  0.58439565,  1.437887  ],\n",
       "       [ 0.533231  , -1.0117726 , -2.316002  ]], dtype=float32), b=Array([-1.5917008, -0.9385306], dtype=float32))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.11168969,  0.58439565,  1.437887  ],\n",
       "       [ 0.533231  , -1.0117726 , -2.316002  ]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.layer1.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the tree structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef(CustomNode(namedtuple[NNParameters], [CustomNode(namedtuple[LayerParameters], [*, *]), CustomNode(namedtuple[LayerParameters], [*, *])]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_structure(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can apply all sorts of tee functions to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNParameters(layer1=LayerParameters(W=Array([[0.01247459, 0.34151828, 2.067519  ],\n",
       "       [0.28433532, 1.0236839 , 5.363865  ]], dtype=float32), b=Array([2.5335114, 0.8808397], dtype=float32)), layer2=LayerParameters(W=Array([[0.19085254, 0.2616532 ]], dtype=float32), b=Array([0.4507926], dtype=float32)))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_map(lambda x: x ** 2, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, to use this with our neural network, we need to be able to convert the dictionaries to named tuples.\n",
    "Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(2.3701239, dtype=float32),\n",
       " NNParameters(layer1=LayerParameters(W=Array([[-0.13561864,  0.05534614,  0.523672  ],\n",
       "        [ 0.15322341, -0.2207033 , -0.43335414]], dtype=float32), b=Array([-0.06904855, -0.31670618], dtype=float32)), layer2=LayerParameters(W=Array([[-0.38613218, -1.1190661 ]], dtype=float32), b=Array([1.1473536], dtype=float32))))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@partial(vmap, in_axes=(0, None))\n",
    "def simple_nn(x, params):\n",
    "    W1 = params.layer1.W\n",
    "    b1 = params.layer1.b\n",
    "    W2 = params.layer2.W\n",
    "    b2 = params.layer2.b\n",
    "    return W2 @ jnp.tanh(W1 @ x + b1) + b2\n",
    "\n",
    "@jit\n",
    "@value_and_grad\n",
    "def loss(params, xs, ys):\n",
    "    pred = simple_nn(xs, params)\n",
    "    squared_error = jnp.mean((pred - ys)**2)\n",
    "    l2_norm = tree_util.tree_reduce(\n",
    "        lambda x, y: jnp.sum(x) + jnp.sum(y),\n",
    "        tree_util.tree_map(lambda x: x ** 2, params),\n",
    "        0.0\n",
    "    )\n",
    "    return squared_error + 0.1 * l2_norm\n",
    "\n",
    "loss(params, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equinox - How to actually do this in practice\n",
    "\n",
    "We don't won't to be building neural networks by hand.\n",
    "There are three main libraries to build neural networks in `Jax`:\n",
    "\n",
    "- [Flax](https://github.com/google/flax).\n",
    "- [Haiku](https://github.com/google-deepmind/dm-haiku).\n",
    "- [Equinox](https://github.com/patrick-kidger/equinox).\n",
    "\n",
    "Equinox is perhaps the simplest one as it relies only on `Pytrees`. It also forces us to inspect the details of the neural network. This is essential for this course. So, we will use Equinox.\n",
    "\n",
    "You should go through [All of Equinox](https://docs.kidger.site/equinox/all-of-equinox/) to learn how to use it.\n",
    "And also some examples like [MNIST](https://docs.kidger.site/equinox/examples/mnist/).\n",
    "Note that we haven't talked about optimization yet.\n",
    "We will do it in another lecture.\n",
    "\n",
    "Here is how the network we built above looks like in Equinox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "\n",
    "\n",
    "class SimpleNN(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, key):\n",
    "        key1, key2 = random.split(key)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(n_inputs, n_hidden, key=key1),\n",
    "            eqx.nn.Linear(n_hidden, n_outputs, key=key2),\n",
    "        ]\n",
    "\n",
    "    # Notice how neatly we can vectorize the forward pass\n",
    "    # Here we need to use in_axes=(None, 0) because the first argument\n",
    "    # is to __call__ is self, which refers to the model itself.\n",
    "    # We don't want to vectorize over this argument.\n",
    "    @partial(vmap, in_axes=(None, 0))\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jnp.tanh(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that the parameters of the network are now in a `Module` object, nicely organized.\n",
    "Here is how we can make such a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  layers=[\n",
       "    Linear(\n",
       "      weight=f32[2,3],\n",
       "      bias=f32[2],\n",
       "      in_features=3,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[1,2],\n",
       "      bias=f32[1],\n",
       "      in_features=2,\n",
       "      out_features=1,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.PRNGKey(314)\n",
    "model = SimpleNN(3, 2, 1, key)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.26482067],\n",
       "       [-0.17487162],\n",
       "       [-0.28958413],\n",
       "       [-0.34640497],\n",
       "       [-0.10561763],\n",
       "       [-0.12498382],\n",
       "       [-0.26911202],\n",
       "       [-0.24482018],\n",
       "       [-0.36088067],\n",
       "       [-0.21855468]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a pytree, see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyTreeDef(CustomNode(SimpleNN[('layers',), (), ()], [[CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (3, 2, True)], [*, *]), CustomNode(Linear[('weight', 'bias'), ('in_features', 'out_features', 'use_bias'), (2, 1, True)], [*, *])]]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get the parameters, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.48662117,  0.08805605,  0.25260752],\n",
       "       [ 0.55680007, -0.21773158, -0.5048137 ]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or all together (but without names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[-0.48662117,  0.08805605,  0.25260752],\n",
       "        [ 0.55680007, -0.21773158, -0.5048137 ]], dtype=float32),\n",
       " Array([-0.04204372, -0.52129227], dtype=float32),\n",
       " Array([[-0.42024657, -0.18588884]], dtype=float32),\n",
       " Array([-0.30617577], dtype=float32)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_leaves(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can get them organized a separate pytree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[Linear(\n",
       "     weight=f32[2,3],\n",
       "     bias=f32[2],\n",
       "     in_features=3,\n",
       "     out_features=2,\n",
       "     use_bias=True\n",
       "   ),\n",
       "   Linear(\n",
       "     weight=f32[1,2],\n",
       "     bias=f32[1],\n",
       "     in_features=2,\n",
       "     out_features=1,\n",
       "     use_bias=True\n",
       "   )]],\n",
       " PyTreeDef(CustomNode(SimpleNN[('layers',), (), ()], [*])))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.tree_flatten_one_level(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you should really think of the model and the parameters as a single object.\n",
    "For example, here is how you can compute the L2 norm of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.4990535, dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_reduce(\n",
    "    lambda x, y: jnp.sum(x) + jnp.sum(y),\n",
    "    tree_util.tree_map(lambda x: x ** 2, model),\n",
    "    0.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "@value_and_grad\n",
    "def loss(model, xs, ys):\n",
    "    pred = model(xs)\n",
    "    squared_error = jnp.mean((pred - ys)**2)\n",
    "    l2_norm = tree_util.tree_reduce(\n",
    "        lambda x, y: jnp.sum(x) + jnp.sum(y),\n",
    "        tree_util.tree_map(lambda x: x ** 2, model),\n",
    "        0.0\n",
    "    )\n",
    "    return squared_error + 0.1 * l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the gradient is with respect to the `model` which is identified with its parameters.\n",
    "Here is how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, g = loss(model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.69529444, dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  layers=[\n",
       "    Linear(\n",
       "      weight=f32[2,3],\n",
       "      bias=f32[2],\n",
       "      in_features=3,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[1,2],\n",
       "      bias=f32[1],\n",
       "      in_features=2,\n",
       "      out_features=1,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the gradient is a pytree with the same structure as the model. Again, if you want to see the actual values, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([[-0.16479225,  0.03449459,  0.04574944],\n",
       "        [ 0.0967427 , -0.03741584, -0.11203536]], dtype=float32),\n",
       " Array([ 0.1264575 , -0.06837945], dtype=float32),\n",
       " Array([[-0.15345962,  0.18075086]], dtype=float32),\n",
       " Array([-0.4314887], dtype=float32)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_util.tree_leaves(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit that it is not trivial to understand, but once you get it, it is very powerful.\n",
    "You can make whatever neural network you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

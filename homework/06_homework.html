
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Homework 6 &#8212; Advanced Scientific Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'homework/06_homework';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Homework 7 - TEMPLATE - DO NOT DO IT YET" href="07_homework.html" />
    <link rel="prev" title="Homework 5" href="05_homework.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Advanced Scientific Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Advanced Scientific Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Advanced Scientific Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml-software/intro.html">Modern Machine Learning Software</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ml-software/functional_programming/00_intro.html">Functional Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/functional_programming/01_primer.html">A Primer on Functional Programming</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/functional_programming/02_jit.html">Just in Time Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/functional_programming/03_vectorization.html">Vectorization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/functional_programming/04_random.html">Pseudo Random Numbers without Side Effects</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ml-software/types_and_models/00_intro.html">Type Systems, Pytrees, and Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/types_and_models/01_why.html">Type Systems and why We Care</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/types_and_models/02_typing.html">Python Type Annotations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/types_and_models/03_haskell.html">Haskell Type System</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/types_and_models/04_pytrees.html">Pytrees to represent model parameters</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ml-software/differentiation/00_intro.html">Differentiable Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/differentiation/01_numerical.html">Numerical Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/differentiation/02_symbolic.html">Symbolic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/differentiation/03_autodiff.html">Automatic Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/differentiation/04_jax_grad.html">Autograd with JAX</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ml-software/optimization/00_intro.html">Optimization for Scientific Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/01_optimization_problems.html">Basics of Optimization Problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/02_gradient_descent.html">Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/03_momentum.html">Gradient Descent with Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/04_optax.html"><code class="docutils literal notranslate"><span class="pre">Optax</span></code> - Optimizers in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/05_sgd.html">Stochastic Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/06_adaptive.html">Optimization Algorithms with Adaptive Learning Rates</a></li>

<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/07_second_order.html">Second-order Methods for Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/08_initialization.html">Initialization of Neural Network Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ml-software/optimization/09_gpu_training.html">Training a neural network on the GPU</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../up/intro.html">Uncertainty Propagation through Scientific Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/sensitivity_analysis/00_intro.html">Sensitivity Analysis of ODEs and PDEs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/01_theory.html">Local Sensitivity Analysis for Ordinary Differential Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/02_diff_ode.html">Differentiating the Solution of Ordinary Differential Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/03_example_ode.html">Example: The Duffing Oscillator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/04_example_lorenz.html">Example: Lorenz System</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/05_fokker_planck.html">Beyond Local Sensitivity Analysis: The Fokker-Planck Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/06_lhs.html">Latin Hypercube Designs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/07_sobol.html">Sobol’s Sequence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/sensitivity_analysis/08_global_sensitivity_analysis.html">Global Sensitivity Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/polynomial_chaos/00_intro.html">Uncertainty Propagation using Polynomial Chaos</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/01_functional_analysis.html">Required Functional Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/02_pc_uniform.html">Symbolic Construction of Polynomial Chaos for Uniform Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/03_pc_hermite.html">Symbolic Construction of Polynomial Chaos for Gaussian Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/04_orthpol_demo.html">Numerical Estimation of Orthogonal Polynomials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/05_pc_ode_1d.html">Using Polynomial Chaos to Propagate Uncertainty through an ODE</a></li>

<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/06_tensor_product.html">Polynomial Chaos in Many Dimensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/07_pce_dynamical_system.html">Uncertainty Propagation in Dynamical Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/polynomial_chaos/08_limitations.html">Limitations of Polynomial Chaos</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/surrogates/intro.html">Surrogates Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/surrogates/01_basics_of_surrogate_models.html">Basic Elements of Surrogate Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/surrogates/02_nn_surrogates.html">Example of a Neural Network Surrogate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/surrogates/03_gp_surrogates.html">Example of Gaussian Process Surrogate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/surrogates/04_gpr_large_data.html">Example – Gaussian Process Regression with Large Datasets</a></li>

</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/mf/intro.html">Multi-fidelity Surrogates</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/mf/01_mf_basics.html">Multifidelity modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/mf/02_mf_gps.html">Multifidelity Gaussian process surrogates</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/al/intro.html">Active Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/al/01_al_basics.html">Active Learning Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/al/02_al.html">Uncertainty Sampling Example</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../up/symmetries/intro.html">Embedding Symmetries in Surrogate Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../up/symmetries/01_symmetries.html">Enforcing Symmetries in Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../up/symmetries/02_e3nn.html">Euclidean Neural Networks</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../hup/intro.html">High-dimensional Uncertainty Propagation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../hup/funcin/intro.html">Functional Inputs to Scientific Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/01_functional_inputs.html">Functional Inputs to Scientific Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/02_svd.html">Singular Value Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/03_pca.html">Connection Between SVD and Principal Component Analysis (PCA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/04_kle.html">The Karhunen-Loève Expansion of a Gaussian Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/05_up_example.html">Example – Surrogate for stochastic heat equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/funcin/06_up_hout_example.html">Example – Surrogate for stochastic heat equation with principal component analysis</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../hup/op/intro.html">Operator Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../hup/op/01_reading.html">Operator Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/op/02_deeponet.html">DeepONet JAX Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hup/op/03_fno.html">FNO JAX Tutorial</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../inverse/intro.html">Inverse Problems in Deterministic Scientific Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/basics/intro.html">Basics of Inverse Problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/basics/01_classic_formulation.html">The Classical Formulation of Inverse Problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/basics/02_classic_example.html">Example – The catalysis Problem using a Classical Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/basics/03_bayesian_formulation.html">Bayesian formulation to inverse problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/basics/04_laplace_approximation_gen.html">The Laplace approximation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/basics/05_laplace_example.html">Example - The Catalysis Problem using the Laplace Approximation</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/sampling/intro.html">Sampling from Posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/sampling/01_mcmc_basics.html">Basics of MCMC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/sampling/02_mcmc_blackjax.html">Metropolis-Hastings with Blackjax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/sampling/03_hmc_blackjax.html">Hamiltonian Monte Carlo with Blackjax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/sampling/04_nuts_blackjax.html">No-U-Turn Sampler with Blackjax</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/vi/intro.html">Variational Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/vi/01_basics.html">Basics of Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/vi/02_catalysis.html">Example – The catalysis problem using variational inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/vi/03_3d_reconstruction.html">Example - 3D particle position reconstrution from images</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/hbayes/intro.html">Hierarchical Bayesian Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/hbayes/01_basics.html">Hierarchical Bayesian modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/hbayes/02_hbayes_example.html">Population uncertainty</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/hbayes/03_bad_post_geometry.html">Improving posterior geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/hbayes/04_amortized_vi.html">Amortized variational inference</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/odes/intro.html">Deterministic, Finite-dimensional, Dynamical Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/odes/01_basics.html">Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/odes/02_identifiability.html">Structural Identifiability of a Harmonic Oscillator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/odes/03_dyn_system_multiple_traj.html">Example - A dynamical system with multiple observed trajectories</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/pdes/intro.html">PDE-constrained inverse problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/pdes/01_reading.html">Calibration of partial differential equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/pdes/02_thermal.html">Inferring thermal conductivity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/pdes/03_contamination.html">Inferring the location of a contaminant</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../inverse/data/intro.html">Data-driven Modeling of Dynamical Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../inverse/data/01_lasso.html">Sparsity Promoting Regularization (L1-regularization or Lasso regression)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/data/02_sindy_1.html">Sparse Identification of Nonlinear Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/data/03_sindy_2.html">SINDy - Example 2: Lorenz system</a></li>
<li class="toctree-l3"><a class="reference internal" href="../inverse/data/04_nodes.html">Neural ODEs</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pinns/intro.html">Physics-informed Neural Networks (PINNs)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pinns/basics/intro.html">Basics of Physics-Informed Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pinns/basics/forward.html">Physics-Informed Neural Networks (PINNs) - Forward Problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pinns/basics/spectral_bias.html">Spectral Bias of Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pinns/basics/energy.html">Energy Functionals</a></li>

</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pinns/parametric/intro.html">PINNS for Parametric Studies</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pinns/parametric/parametric_example.html">Solving Parametric Problems using Physics-informed Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pinns/parametric/physics-informed_nops.html">Example - Physics-informed Neural Operators</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pinns/inverse/intro.html">PINNS for Inverse Problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pinns/inverse/01_pinns_inverse_example.html">PINNs - Example of inverse problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pinns/inverse/02_example_Bayesian_pinns.html">Example - Bayesian PINNs</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../sinverse/intro.html">Inverse Problems in Stochastic Scientific Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../sinverse/sodes/intro.html">Stochastic Differential Equations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/sodes/01_reading.html">Stochastic differential equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/sodes/02_bm.html">Example - Brownian Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/sodes/03_stochastic_exponential_growth.html">Example - Stochastic Exponetial Growth</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/sodes/04_ornstein_uhlenbeck.html">Example - Ornstein-Uhlenbeck Process</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../sinverse/fs/intro.html">Filtering and Smoothing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/fs/01_reading.html">Particle Filters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/fs/02_filter.html">Example - Particle filters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/fs/03_smoother.html">Example - Particle smoother</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../sinverse/cali/intro.html">State Estimation and Parameter Calibration</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/cali/01_em_theory.html">Parameter Estimation in Stochastic Differential Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/cali/02_em_example.html">Example - System identification with expectation maximization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/cali/03_pmcmc_theory.html">Bayesian Inference in State-space Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sinverse/cali/04_pmcmc_example.html">Example - System identification with particle MCMC</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_intro.html">Homework Problems</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_homework.html">Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_homework.html">Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_homework.html">Homework 3</a></li>




<li class="toctree-l2"><a class="reference internal" href="04_homework.html">Homework 4</a></li>


<li class="toctree-l2"><a class="reference internal" href="05_homework.html">Homework 5</a></li>




<li class="toctree-l2 current active"><a class="current reference internal" href="#">Homework 6</a></li>



<li class="toctree-l2"><a class="reference internal" href="07_homework.html">Homework 7 - TEMPLATE - DO NOT DO IT YET</a></li>

</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/PredictiveScienceLab/advanced-scientific-machine-learning/blob/master/book/homework/06_homework.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/homework/06_homework.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Homework 6</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Homework 6</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#student-details">Student details</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-why-does-the-metropolis-algorithm-work">Problem 1 - Why does the Metropolis algorithm work</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-prove-that-detailed-balance-implies-stationarity">Part A - Prove that detailed balance implies stationarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-the-metropolis-hastings-transition-kernel">Part B - The Metropolis-Hastings transition kernel</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-i-derive-the-transition-kernel-of-the-metropolis-algorithm">B.I - Derive the transition kernel of the Metropolis algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-ii-show-that-the-transition-kernel-satisfies-the-detailed-balance-condition">B.II - Show that the transition kernel satisfies the detailed balance condition</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-mathematics-of-variational-inference">Problem 2 - Mathematics of Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-parameterization-of-a-covariance-matrix">Part A - Parameterization of a covariance matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-i-show-that-sigma-is-positive-semi-definite">A.I - Show that <span class="math notranslate nohighlight">\(\Sigma\)</span> is positive semi-definite.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-ii-numerical-exploration-of-a-rank-k-covariance-matrix">A.II - Numerical exploration of a rank-<span class="math notranslate nohighlight">\(k\)</span> covariance matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-iii-low-rank-approximation-that-is-actually-positive-definite">A.III - Low-rank approximation that is actually positive definite</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-multi-point-convexity">Part B - Multi-point convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-jensen-s-inequality">Part C - Jensen’s inequality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-non-negativity-of-the-kl-divergence">Part D - Non-negativity of the KL divergence</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-calibrating-a-pharmacokinetic-model">Problem 3 - Calibrating a pharmacokinetic model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-implement-the-unnormalized-log-posterior-density">Part A - Implement the (unnormalized) log posterior density</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-plot-the-prior-predictive-distribution">Part B - Plot the prior predictive distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-find-the-maximum-a-posteriori-estimate">Part C - Find the maximum a-posteriori estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-sample-the-posterior-with-mcmc">Part D - Sample the posterior with MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-variational-inference-with-mean-field-gaussian-guide">Part E - Variational inference with mean-field Gaussian guide</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-f-variational-inference-with-full-rank-gaussian-guide">Part F - Variational inference with full-rank Gaussian guide</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_hide-input tag_hide-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;paper&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;ticks&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="homework-6">
<h1>Homework 6<a class="headerlink" href="#homework-6" title="Link to this heading">#</a></h1>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul>
<li><p>Module 5: Inverse problems in deterministic scientifc models</p>
<ul class="simple">
<li><p>Inverse problems basics</p></li>
<li><p>Sampling from posteriors</p></li>
<li><p>Variational inference</p></li>
<li><p>Deterministic, finite-dimensional dynamical systems</p></li>
</ul>
 <!-- - PDE-constrained inverse problems -->
 <!-- - Purely data-driven learning of dynamical systems -->
</li>
</ul>
</section>
<section id="instructions">
<h2>Instructions<a class="headerlink" href="#instructions" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Type your name and email in the “Student details” section below.</p></li>
<li><p>Develop the code and generate the figures you need to solve the problems using this notebook.</p></li>
<li><p>For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.</p></li>
<li><p>The total homework points are 100. Please note that the problems are not weighed equally.</p></li>
</ul>
</section>
<section id="student-details">
<h2>Student details<a class="headerlink" href="#student-details" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>First Name:</strong></p></li>
<li><p><strong>Last Name:</strong></p></li>
<li><p><strong>Email:</strong></p></li>
<li><p><strong>Used generative AI to complete this assignment (Yes/No):</strong></p></li>
<li><p><strong>Which generative AI tool did you use (if applicable)?:</strong></p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="problem-1-why-does-the-metropolis-algorithm-work">
<h1>Problem 1 - Why does the Metropolis algorithm work<a class="headerlink" href="#problem-1-why-does-the-metropolis-algorithm-work" title="Link to this heading">#</a></h1>
<p>The objective of this problem is to understand why the Metropolis algorithm works.</p>
<p>Consider a Markov chain <span class="math notranslate nohighlight">\(x_n\)</span> with transition probabilities <span class="math notranslate nohighlight">\(p(x_{n+1}|x_n)\)</span> and a probability density <span class="math notranslate nohighlight">\(\pi(x)\)</span>.
We say that <span class="math notranslate nohighlight">\(x_n\)</span> has stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> if:</p>
<div class="math notranslate nohighlight">
\[
\pi(x_{n+1}) = \int p(x_{n+1}|x_n)\pi(x_n)dx_n.
\]</div>
<p>Intuitively, we can think of the equation above as follows.
If we, somehow, sample <span class="math notranslate nohighlight">\(x_n\)</span> from <span class="math notranslate nohighlight">\(\pi\)</span> and then sample <span class="math notranslate nohighlight">\(x_{n+1}\)</span> from the transition probability <span class="math notranslate nohighlight">\(p(x_{n+1}|x_n)\)</span>, then <span class="math notranslate nohighlight">\(x_{n+1}\)</span> is also a sample from <span class="math notranslate nohighlight">\(\pi(x)\)</span>.
It is like once we have a sample <span class="math notranslate nohighlight">\(\pi\)</span> sampling the Markov chain keeps giving us samples from <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>We say that the Markov chain <span class="math notranslate nohighlight">\(x_n\)</span> is <em>reversible</em> with respect to <span class="math notranslate nohighlight">\(\pi\)</span> (equivalently, satisfies the <em>detailed balance</em> condition) with respect to <span class="math notranslate nohighlight">\(\pi\)</span>, if:</p>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}|x_n)\pi(x_n) = p(x_n|x_{n+1})\pi(x_{n+1}).
\]</div>
<p>Intuitively, this condition means that going from sampling <span class="math notranslate nohighlight">\(x_{n}\)</span> from <span class="math notranslate nohighlight">\(\pi\)</span> and transition to <span class="math notranslate nohighlight">\(x_{n+1}\)</span> has the same probability as doing the inverse.</p>
<section id="part-a-prove-that-detailed-balance-implies-stationarity">
<h2>Part A - Prove that detailed balance implies stationarity<a class="headerlink" href="#part-a-prove-that-detailed-balance-implies-stationarity" title="Link to this heading">#</a></h2>
<p>Suppose that the Markov chain <span class="math notranslate nohighlight">\(x_n\)</span> satisfies the detailed balance condition with respect to <span class="math notranslate nohighlight">\(\pi\)</span>. Prove that <span class="math notranslate nohighlight">\(\pi\)</span> is a stationary distribution of the Markov chain.</p>
<p><strong>Answer:</strong></p>
<p><em>Your answer here</em></p>
</section>
<section id="part-b-the-metropolis-hastings-transition-kernel">
<h2>Part B - The Metropolis-Hastings transition kernel<a class="headerlink" href="#part-b-the-metropolis-hastings-transition-kernel" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\pi(x)\)</span> be the target distribution.
Let <span class="math notranslate nohighlight">\(q(\tilde{x}_{n+1}|x_n)\)</span> be a proposal distribution of the Metropolis-Hastings algorithm.</p>
<p>The Metropolis-Hastings algorithm results in a Markov chain <span class="math notranslate nohighlight">\(x_n\)</span> defined as follows:</p>
<ul class="simple">
<li><p>Sample <span class="math notranslate nohighlight">\(\tilde{x}_{n+1} \sim q(\tilde{x}_{n+1}|x_n)\)</span></p></li>
<li><p>Accept <span class="math notranslate nohighlight">\(\tilde{x}_{n+1}\)</span> and set <span class="math notranslate nohighlight">\(x_{n+1} = \tilde{x}_{n+1}\)</span> with probability <span class="math notranslate nohighlight">\(\alpha(x_n, \tilde{x}_{n+1})\)</span></p></li>
<li><p>Reject <span class="math notranslate nohighlight">\(\tilde{x}_{n+1}\)</span> and set <span class="math notranslate nohighlight">\(x_{n+1} = x_n\)</span> with probability <span class="math notranslate nohighlight">\(1-\alpha(x_n, \tilde{x}_{n+1}),\)</span></p></li>
</ul>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\alpha(x_n, \tilde{x}_{n+1}) = \min\left(1, \frac{\pi(\tilde{x}_{n+1})q(x_n|\tilde{x}_{n+1})}{\pi(x_n)q(\tilde{x}_{n+1}|x_n)}\right).
\]</div>
<p>The purpose of this problem is to show that the transition kernel of the resulting Markov chain satisfies the detailed balance condition with respect to <span class="math notranslate nohighlight">\(\pi\)</span>, and thus <span class="math notranslate nohighlight">\(\pi\)</span> is its stationary distribution.</p>
<section id="b-i-derive-the-transition-kernel-of-the-metropolis-algorithm">
<h3>B.I - Derive the transition kernel of the Metropolis algorithm<a class="headerlink" href="#b-i-derive-the-transition-kernel-of-the-metropolis-algorithm" title="Link to this heading">#</a></h3>
<p>Show that the transition kernel of the Metropolis algorithm is:</p>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}|x_n) = \alpha(x_n, x_{n+1})q(x_{n+1}|x_n) +
\delta(x_{n+1} - x_n)\int (1 - \alpha(x_n, \tilde{x}_{n+1}))q(\tilde{x}_{n+1}|x_n)d\tilde{x}_{n+1},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta\)</span> is the Dirac delta function.</p>
<p>Hints:</p>
<ul class="simple">
<li><p>Introduce an intermediate variable <span class="math notranslate nohighlight">\(i\)</span> that takes the value <span class="math notranslate nohighlight">\(1\)</span> if the proposed move is accepted and <span class="math notranslate nohighlight">\(0\)</span> otherwise. That is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
i | x_n, \tilde{x}_{n+1} \sim \begin{cases}
    1 &amp; \text{with probability } \alpha(x_n, \tilde{x}_{n+1}) \\
    0 &amp; \text{with probability } 1 - \alpha(x_n, \tilde{x}_{n+1}).
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Write the joint distribution <span class="math notranslate nohighlight">\(p(x_{n+1}| i, x_n, \tilde{x}_{n+1})\)</span> in terms of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(\tilde{x}_{n+1}\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}| i, x_n, \tilde{x}_{n+1}) = [\delta(x_{n+1} - \tilde{x}_{n+1})]^i [\delta(x_{n+1} - x_n)]^{1-i}.
\]</div>
<ul class="simple">
<li><p>Use the sum rule to express <span class="math notranslate nohighlight">\(p(x_{n+1}|x_n)\)</span> in terms of <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(\tilde{x}_{n+1}\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}|x_n) = \int \sum_i p(x_{n+1}| i, x_n, \tilde{x}_{n+1}) p(i | x_n, \tilde{x}_{n+1}) q(\tilde{x}_{n+1}|x_n) d\tilde{x}_{n+1}.
\]</div>
<ul class="simple">
<li><p>Use the definition of the Dirac delta function to simplify the expression.</p></li>
</ul>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
<section id="b-ii-show-that-the-transition-kernel-satisfies-the-detailed-balance-condition">
<h3>B.II - Show that the transition kernel satisfies the detailed balance condition<a class="headerlink" href="#b-ii-show-that-the-transition-kernel-satisfies-the-detailed-balance-condition" title="Link to this heading">#</a></h3>
<p>Show that the transition kernel of the Metropolis algorithm satisfies the detailed balance condition with respect to <span class="math notranslate nohighlight">\(\pi\)</span>, and thus <span class="math notranslate nohighlight">\(\pi\)</span> is its stationary distribution.
Mathematically, you need to show that:</p>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}|x_n) \pi(x_n) = p(x_n|x_{n+1}) \pi(x_{n+1}).
\]</div>
<p>Hints:</p>
<ul class="simple">
<li><p>First prove that <span class="math notranslate nohighlight">\(a(x_n, x_{n+1})q(x_{n+1}|x_n)\pi(x_n) = a(x_{n+1}, x_n)q(x_n|x_{n+1})\pi(x_{n+1})\)</span>.</p></li>
<li><p>Then, reuse the result above the symmetry of the Dirac delta function.</p></li>
</ul>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="problem-2-mathematics-of-variational-inference">
<h1>Problem 2 - Mathematics of Variational Inference<a class="headerlink" href="#problem-2-mathematics-of-variational-inference" title="Link to this heading">#</a></h1>
<section id="part-a-parameterization-of-a-covariance-matrix">
<h2>Part A - Parameterization of a covariance matrix<a class="headerlink" href="#part-a-parameterization-of-a-covariance-matrix" title="Link to this heading">#</a></h2>
<p>The purpose is to show that the commonly used rank-<span class="math notranslate nohighlight">\(k\)</span> parameterization of the covariance matrix is indeed positive definite.</p>
<p>Let <span class="math notranslate nohighlight">\(k\)</span> be a positive integer, and <span class="math notranslate nohighlight">\(\lambda_1, \dots, \lambda_k\)</span> be real numbers.
Let <span class="math notranslate nohighlight">\(d\)</span> be another positive integer (the dimension of the covariance matrix) with <span class="math notranslate nohighlight">\(d \geq k\)</span>.
Let <span class="math notranslate nohighlight">\(u_1, \dots, u_k\)</span> be <span class="math notranslate nohighlight">\(d\)</span>-dimensional vectors, not necessarily orthogonal, but linearly independent.</p>
<p>Consider the following matrix:</p>
<div class="math notranslate nohighlight">
\[
\Sigma = \sum_{i=1}^k e^{\lambda_i} u_i u_i^\top.
\]</div>
<section id="a-i-show-that-sigma-is-positive-semi-definite">
<h3>A.I - Show that <span class="math notranslate nohighlight">\(\Sigma\)</span> is positive semi-definite.<a class="headerlink" href="#a-i-show-that-sigma-is-positive-semi-definite" title="Link to this heading">#</a></h3>
<p>Hint: You need to show that for any non-zero vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^d\)</span>, the quadratic form <span class="math notranslate nohighlight">\(x^\top \Sigma x \geq 0\)</span>.</p>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
<section id="a-ii-numerical-exploration-of-a-rank-k-covariance-matrix">
<h3>A.II - Numerical exploration of a rank-<span class="math notranslate nohighlight">\(k\)</span> covariance matrix<a class="headerlink" href="#a-ii-numerical-exploration-of-a-rank-k-covariance-matrix" title="Link to this heading">#</a></h3>
<p>Set <span class="math notranslate nohighlight">\(d=100\)</span> and <span class="math notranslate nohighlight">\(k=10\)</span>.
Randomly generate <span class="math notranslate nohighlight">\(u_1, \dots, u_k\)</span> from the standard normal distribution.
Randomly generate <span class="math notranslate nohighlight">\(\lambda_1, \dots, \lambda_k\)</span> from the standard normal distribution.
Write Jax code (without a loop) to form the matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> as defined above.
Generate a random <span class="math notranslate nohighlight">\(\Sigma\)</span> and plot the eigenvalues.
Are they all non-negative?
What is the determinant of <span class="math notranslate nohighlight">\(\Sigma\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># as many code blocks and markdown blocks as you want</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="a-iii-low-rank-approximation-that-is-actually-positive-definite">
<h3>A.III - Low-rank approximation that is actually positive definite<a class="headerlink" href="#a-iii-low-rank-approximation-that-is-actually-positive-definite" title="Link to this heading">#</a></h3>
<p>In the previous part, we saw that the rank-<span class="math notranslate nohighlight">\(k\)</span> approximation is not positive definite.
To fix it, we typically use this parameterization instead:</p>
<div class="math notranslate nohighlight">
\[
\Sigma = \sum_{i=1}^k \lambda_i u_i u_i^\top + \text{diag}(e^{\theta_1}, \dots, e^{\theta_d}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta_1, \dots, \theta_d\)</span> are real numbers.</p>
<p>Modify your Jax code and generate a random <span class="math notranslate nohighlight">\(\Sigma\)</span> using this parameterization.
Plot the eigenvalues.
Are they all non-negative?
What is the determinant of <span class="math notranslate nohighlight">\(\Sigma\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># as many code blocks and markdown blocks as you want</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="part-b-multi-point-convexity">
<h2>Part B - Multi-point convexity<a class="headerlink" href="#part-b-multi-point-convexity" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> be a convex function.
Let <span class="math notranslate nohighlight">\(x_1, \dots, x_n \in \mathbb{R}^d\)</span> be <span class="math notranslate nohighlight">\(n\)</span> points.
Let <span class="math notranslate nohighlight">\(w_1, \dots, w_n \in \mathbb{R}\)</span> be <span class="math notranslate nohighlight">\(n\)</span> weights.</p>
<p>Show that:</p>
<div class="math notranslate nohighlight">
\[
f\left(\sum_{i=1}^n w_i x_i\right) \leq \sum_{i=1}^n w_i f(x_i).
\]</div>
<p>Hint: Use the definition of convexity and induction.</p>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
<section id="part-c-jensen-s-inequality">
<h2>Part C - Jensen’s inequality<a class="headerlink" href="#part-c-jensen-s-inequality" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> be a convex function that is continuous.
Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable with values in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>.</p>
<p>Show that:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)].
\]</div>
<p>Hint: Use Part B and the law of large numbers.</p>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
<section id="part-d-non-negativity-of-the-kl-divergence">
<h2>Part D - Non-negativity of the KL divergence<a class="headerlink" href="#part-d-non-negativity-of-the-kl-divergence" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> be two probability distributions on <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>.
Show that the KL divergence <span class="math notranslate nohighlight">\(D_{KL}(p\|q)\)</span> is always non-negative.</p>
<p>Hint: Use the fact that <span class="math notranslate nohighlight">\(-\log\)</span> is a convex function and Jensen’s inequality.</p>
<p><strong>Answer:</strong>
<em>Your answer here</em></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="problem-3-calibrating-a-pharmacokinetic-model">
<h1>Problem 3 - Calibrating a pharmacokinetic model<a class="headerlink" href="#problem-3-calibrating-a-pharmacokinetic-model" title="Link to this heading">#</a></h1>
<p>A pharmacokinetic (PK) compartment model is a set of ordinary differential equations that describe drug transport in the body.
Typically, the body is divided into separate “compartments” (e.g., blood, peripheral tissues) and the transfer of the drug between these compartments is assumed to follow first-order kinetics.
Consider the following two-compartment model for an <em>intravenous (IV) bolus</em> administered drug:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{dc_1}{dt} &amp;= - k_{12} c_1 + k_{21} c_2\\
\frac{dc_2}{dt} &amp;= k_{12} c_1 - (k_{21} + k_d) c_2 \\
c_1(0) &amp;= \frac{m_\text{dose}}{V} \\
c_2(0) &amp;= 0
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(c_1\)</span> is the concentration of the drug in the <em>central compartment</em> (e.g., blood, kidney, liver),
<span class="math notranslate nohighlight">\(c_2\)</span> is the concentration in the <em>peripheral compartment</em> (e.g., muscle, fat),
<span class="math notranslate nohighlight">\(k_i\)</span> are the <em>rate constants</em>,
<span class="math notranslate nohighlight">\(m_\text{dose}\)</span> is the mass of the drug administered,
and <span class="math notranslate nohighlight">\(V\)</span> is the <em>volume of distribution</em>.</p>
<p>(In this context “IV” means the drug is injected directly into the bloodstream, and “bolus” means the drug is given all at once (instead of slowly administering it over minutes/hours).)</p>
<img src="https://raw.githubusercontent.com/PredictiveScienceLab/advanced-scientific-machine-learning/refs/heads/main/book/images/compartment_model.png" alt="pk_model" width="400"/>
<p>Here is an analytic solver for the PK model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">tree</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="nd">@partial</span><span class="p">(</span><span class="n">vmap</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">solve_pk_iv_bolus</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="p">):</span>
    <span class="n">k12</span><span class="p">,</span> <span class="n">k21</span><span class="p">,</span> <span class="n">kd</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">]</span>
    <span class="n">lam1</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">k12</span> <span class="o">+</span> <span class="n">k21</span> <span class="o">+</span> <span class="n">kd</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">k12</span> <span class="o">+</span> <span class="n">k21</span> <span class="o">+</span> <span class="n">kd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">k12</span><span class="o">*</span><span class="n">kd</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">lam2</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">k12</span> <span class="o">+</span> <span class="n">k21</span> <span class="o">+</span> <span class="n">kd</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">k12</span> <span class="o">+</span> <span class="n">k21</span> <span class="o">+</span> <span class="n">kd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">k12</span><span class="o">*</span><span class="n">kd</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="n">dosage_mass</span> <span class="o">/</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">lam1</span> <span class="o">+</span> <span class="n">k12</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">lam2</span> <span class="o">+</span> <span class="n">k12</span><span class="p">))</span>
    <span class="n">B</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">lam1</span> <span class="o">+</span> <span class="n">k12</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">lam2</span> <span class="o">+</span> <span class="n">k12</span><span class="p">)</span><span class="o">*</span><span class="n">A</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lam1</span><span class="o">*</span><span class="n">time</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lam2</span><span class="o">*</span><span class="n">time</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c1</span>
</pre></div>
</div>
</div>
</div>
<p>It accepts a dictionary of parameters, the vector of times, and an initial condition:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_params_test</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="mf">10.0</span><span class="p">}</span>
<span class="n">_times_test</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">_dosage_mass_test</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">_c1</span> <span class="o">=</span> <span class="n">solve_pk_iv_bolus</span><span class="p">(</span><span class="n">_params_test</span><span class="p">,</span> <span class="n">_times_test</span><span class="p">,</span> <span class="n">_dosage_mass_test</span><span class="p">)</span>
<span class="n">_c1</span>
</pre></div>
</div>
</div>
</div>
<p>Suppose a subject has received a dose of acetaminophen via IV bolus administration, and we have measured the drug concentration in the blood at discrete times.
Let’s import these data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>curl<span class="w"> </span>-O<span class="w"> </span><span class="s1">&#39;https://raw.githubusercontent.com/PredictiveScienceLab/advanced-scientific-machine-learning/refs/heads/main/book/data/pk/iv_bolus_data_single_patient.json&#39;</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;iv_bolus_data_single_patient.json&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;times&#39;</span><span class="p">])</span>
<span class="n">concentrations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;concentrations&#39;</span><span class="p">])</span>
<span class="n">dosage_mass</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;dosage_mass&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the observation times (in hours), <span class="math notranslate nohighlight">\(\mathbf{t}=(t_1, \dots, t_N)\in\mathbb{R}_+^N\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">times</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the observed concentrations at each time point (in gram/liter), <span class="math notranslate nohighlight">\(\mathbf{y}=(y_1, \dots, y_N) \in \mathbb{R}_+^N\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">concentrations</span>
</pre></div>
</div>
</div>
</div>
<p>And here is the dose (in grams), <span class="math notranslate nohighlight">\(m_\text{dose}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dosage_mass</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the PK data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">concentrations</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Observed concentration&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Concentration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="part-a-implement-the-unnormalized-log-posterior-density">
<h2>Part A - Implement the (unnormalized) log posterior density<a class="headerlink" href="#part-a-implement-the-unnormalized-log-posterior-density" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\theta=(k_{12}, k_{21}, k_d, V) \in \mathbb{R}_+^4\)</span> be the PK parameters, to which we’ll assign weakly-informative priors</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
k_{12} &amp;\sim \text{Exp}(0.1)\\
k_{21} &amp;\sim \text{Exp}(0.1)\\
k_d &amp;\sim \text{Exp}(0.1)\\
V &amp;\sim \text{LogNormal}(4, 0.5).
\end{align*}
\end{split}\]</div>
<p>Also, let <span class="math notranslate nohighlight">\(c_1(t; \theta, m_\text{dose}) \in \mathbb{R}_+^3\)</span> be the concentration in the central (bloodstream) compartment at time <span class="math notranslate nohighlight">\(t\)</span> for initial condition <span class="math notranslate nohighlight">\(x_0=(m_\text{dose}, 0) \in \mathbb{R}_+^2\)</span>.</p>
<p>Assume the observations are independent, identically distributed Gaussian random variables, i.e.,</p>
<div class="math notranslate nohighlight">
\[
y_i|t_i, \theta, \sigma \sim \text{Normal}\Big(\underbrace{c_1(t_i; \theta, x_0)}_\text{ODE solver output}, \sigma^2 I\Big).
\]</div>
<p>Suppose you know, from previous studies, that the measurement uncertainty is <span class="math notranslate nohighlight">\(\sigma=0.001\)</span>.
The (unnormalized) log posterior density function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \underbrace{\log p(\theta|\mathbf{t}, \mathbf{y}, \sigma)}_\text{posterior}
    &amp;= \underbrace{\log p(\theta)}_\text{prior} + \underbrace{\log \prod_{i=1}^N p(y_i | t_i, \theta, \sigma)}_\text{likelihood} + \underbrace{[\text{constant terms w.r.t. } \theta]}_\text{normalizing contant (we can ignore)} \\
    &amp;\propto \log p(k_{12}) + \log p(k_{21}) + \log p(k_d) +\log p(V) + \sum_{i=1}^N \log p(y_i | t_i, \theta, \sigma).
\end{align*}
\end{split}\]</div>
<p>where we are defining <span class="math notranslate nohighlight">\(\propto\)</span> to mean “equal up to a normalizing constant”.
Now, it is easier to work with a set of “unconstrained model parameters” <span class="math notranslate nohighlight">\(\xi\)</span> that span all of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>.
To this end, define</p>
<div class="math notranslate nohighlight">
\[
\xi = \log \theta = (\log k_{12}, \log k_{21}, \log k_d, \log V) \in \mathbb{R}^4.
\]</div>
<p><strong>Your task is to implement the function that computes the (unnormalized) log posterior over <span class="math notranslate nohighlight">\(\xi\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \log p(\xi | \mathbf{t}, \mathbf{y}, \sigma) 
    &amp;= \log p(\theta | \mathbf{t}, \mathbf{y}, \sigma) + \log \det \left| \frac{\partial \theta}{\partial \xi} \right|\\
    &amp;= \log p(\theta | \mathbf{t}, \mathbf{y}, \sigma) + \log \prod_{i=1}^d \frac{\partial \theta_i}{\partial \xi_i} \\
    &amp;= \log p(\xi) + \log p(\mathbf{y} | \mathbf{t}, \xi, \sigma) + \sum_{i=1}^d \xi_i + \underbrace{\text{constant terms}}_\text{ignore these}.
\end{align*}
\end{split}\]</div>
<p>We’ve started it for you—just fill in the missing pieces of the code below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="k">def</span> <span class="nf">constrain</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constrain the parameters to be positive.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">]),</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">unconstrain</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Unconstrain the parameters to be real numbers.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">]),</span>
        <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">]),</span>
    <span class="p">}</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">times</span><span class="p">,</span> <span class="n">concentrations</span><span class="o">=</span><span class="n">concentrations</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="o">=</span><span class="n">dosage_mass</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log likelihood function for a single individual&#39;s PK data.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    xi: dict</span>
<span class="sd">        Dictionary containing the unconstrained parameters of the model:</span>
<span class="sd">        - k12: transfer rate from compartment 1 to compartment 2</span>
<span class="sd">        - k21: transfer rate from compartment 2 to compartment 1</span>
<span class="sd">        - kd: elimination rate from compartment 2</span>
<span class="sd">        - V: volume of distribution</span>
<span class="sd">    times: array</span>
<span class="sd">        Time points at which to evaluate the solution.</span>
<span class="sd">    concentrations: array</span>
<span class="sd">        Observed concentrations at the specified time points.</span>
<span class="sd">    dosage_mass: float</span>
<span class="sd">        Mass of the dosage administered.</span>
<span class="sd">    sigma: float</span>
<span class="sd">        Standard deviation of the measurement noise.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Log likelihood of the observed data given the model parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Log PDF functions for standard distributions</span>
    <span class="n">normal_log_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">lognormal_log_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">exponential_log_pdf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span> <span class="o">-</span> <span class="n">rate</span><span class="o">*</span><span class="n">x</span>
    
    <span class="c1"># Transform to constrained space</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">constrain</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>

    <span class="c1"># Prior</span>
    <span class="n">k12_log_prior</span> <span class="o">=</span> <span class="n">exponential_log_pdf</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">k21_log_prior</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>
    <span class="n">kd_log_prior</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>
    <span class="n">V_log_prior</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>
    <span class="n">log_prior</span> <span class="o">=</span> <span class="n">k12_log_prior</span> <span class="o">+</span> <span class="n">k21_log_prior</span> <span class="o">+</span> <span class="n">kd_log_prior</span> <span class="o">+</span> <span class="n">V_log_prior</span>
    
    <span class="c1"># Likelihood</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">solve_pk_iv_bolus</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>

    <span class="c1"># Determinant of the Jacobian of the transformation</span>
    <span class="n">log_det_jac</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>

    <span class="k">return</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">log_likelihood</span> <span class="o">+</span> <span class="n">log_det_jac</span>
</pre></div>
</div>
</div>
</div>
<p>Your implementation of <span class="math notranslate nohighlight">\(p(\xi|\mathbf{t}, \mathbf{y}, \sigma)\)</span> above could also be done with a probabilistic programming framework.
Here is how to do it with <a class="reference external" href="https://num.pyro.ai/en/latest/index.html#">Numpyro</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jr</span>
<span class="kn">import</span> <span class="nn">numpyro</span>
<span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">numpyro.infer</span> <span class="kn">import</span> <span class="n">util</span>

<span class="k">def</span> <span class="nf">numpyro_model</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">concentrations</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model function for the JAX-based probabilistic model.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    times: array</span>
<span class="sd">        Time points at which to evaluate the solution.</span>
<span class="sd">    concentrations: array</span>
<span class="sd">        Observed concentrations at the specified time points.</span>
<span class="sd">    dosage_mass: float</span>
<span class="sd">        Mass of the dosage administered.</span>
<span class="sd">    sigma: float</span>
<span class="sd">        Standard deviation of the measurement noise.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Log likelihood of the observed data given the model parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Prior</span>
    <span class="n">k12</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k12&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">k21</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;k21&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">kd</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;kd&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>

    <span class="c1"># Likelihood</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="n">k12</span><span class="p">,</span> <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="n">k21</span><span class="p">,</span> <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="n">kd</span><span class="p">,</span> <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">V</span><span class="p">}</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">solve_pk_iv_bolus</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">plate</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">)):</span>
        <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">concentrations</span><span class="p">)</span>

<span class="n">model_default_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">concentrations</span><span class="p">,</span> <span class="n">dosage_mass</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="p">(</span>
    <span class="n">init_params</span><span class="p">,</span>
    <span class="n">potential_fn_gen</span><span class="p">,</span> 
    <span class="n">postprocess_fn_gen</span><span class="p">,</span> 
    <span class="n">model_trace</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">initialize_model</span><span class="p">(</span>
    <span class="n">jr</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">numpyro_model</span><span class="p">,</span>
    <span class="n">model_args</span><span class="o">=</span><span class="n">model_default_args</span><span class="p">,</span>  <span class="c1"># Dummy arguments</span>
    <span class="n">dynamic_args</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the probability density.</span>
<span class="c1"># This is p(ξ|y)</span>
<span class="n">log_posterior_numpyro</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">potential_fn_gen</span><span class="p">(</span><span class="o">*</span><span class="n">model_default_args</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Get the transformation function.</span>
<span class="c1"># This is ξ ↦ θ</span>
<span class="n">constrain_numpyro</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">util</span><span class="o">.</span><span class="n">constrain_fn</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">,</span> <span class="n">model_default_args</span><span class="p">,</span> <span class="p">{},</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># And get the inverse transformation function.</span>
<span class="c1"># This is θ ↦ ξ</span>
<span class="n">unconstrain_numpyro</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">util</span><span class="o">.</span><span class="n">unconstrain_fn</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">,</span> <span class="n">model_default_args</span><span class="p">,</span> <span class="p">{},</span> <span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Run the cell below to test your implementation of <span class="math notranslate nohighlight">\(p(\xi|\mathbf{t}, \mathbf{y}, \sigma)\)</span> against the Numpyro implementation. The printed results should be identical.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="mf">10.0</span><span class="p">,</span>
<span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Log posterior evaluated from scratch: &#39;</span><span class="p">,</span> <span class="n">log_posterior</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Log posterior evaluated from numpyro: &#39;</span><span class="p">,</span> <span class="n">log_posterior_numpyro</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-b-plot-the-prior-predictive-distribution">
<h2>Part B - Plot the prior predictive distribution<a class="headerlink" href="#part-b-plot-the-prior-predictive-distribution" title="Link to this heading">#</a></h2>
<p>First, let’s create a function that samples the random variable <span class="math notranslate nohighlight">\(\xi\)</span> (i.e., the prior over the unnormalized variables).</p>
<p>Again, complete the missing pieces of the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@partial</span><span class="p">(</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sample_prior_xi</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">k12</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">0.1</span><span class="p">)</span><span class="o">*</span><span class="n">jr</span><span class="o">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,))</span>
    <span class="n">k21</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>
    <span class="n">kd</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Your code here</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">*</span><span class="n">jr</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,),</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;k12&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">k12</span><span class="p">),</span>
        <span class="s1">&#39;k21&#39;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span>  <span class="c1"># Your code here</span>
        <span class="s1">&#39;kd&#39;</span><span class="p">:</span> <span class="o">...</span><span class="p">,</span>  <span class="c1"># Your code here</span>
        <span class="s1">&#39;V&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">V</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">xi</span>
</pre></div>
</div>
</div>
</div>
<p>And again, we could implement the same thing using Numpyro.
Here is how you could use Numpyro’s <code class="docutils literal notranslate"><span class="pre">Predictive</span></code> class to create a function that samples all latent variables defined in <code class="docutils literal notranslate"><span class="pre">numpyro_model</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@partial</span><span class="p">(</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sample_prior_xi_numpyro</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">infer</span><span class="o">.</span><span class="n">Predictive</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)(</span><span class="n">key</span><span class="p">,</span> <span class="o">*</span><span class="n">model_default_args</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">unconstrain_numpyro</span><span class="p">)(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">xi</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">init_params</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="k">return</span> <span class="n">xi</span>
</pre></div>
</div>
</div>
</div>
<p>Run the following cell to test your prior sampler against Numpyro’s prior sampler.
The two should be essentially equivalent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi_prior_samples</span> <span class="o">=</span> <span class="n">sample_prior_xi</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">100_000</span><span class="p">)</span>
<span class="n">xi_prior_samples_numpyro</span> <span class="o">=</span> <span class="n">sample_prior_xi_numpyro</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">100_000</span><span class="p">)</span>
<span class="n">param_prior_samples</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">constrain</span><span class="p">)(</span><span class="n">xi_prior_samples</span><span class="p">)</span>
<span class="n">param_prior_samples_numpyro</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">constrain_numpyro</span><span class="p">)(</span><span class="n">xi_prior_samples_numpyro</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $\xi_{k_</span><span class="si">{12}</span><span class="s1">}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $\xi_{k_</span><span class="si">{21}</span><span class="s1">}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $\xi_{k_</span><span class="si">{d}</span><span class="s1">}$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">xi_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $\xi_</span><span class="si">{V}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;k12&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $k_</span><span class="si">{12}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;k21&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $k_</span><span class="si">{21}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;kd&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $k_</span><span class="si">{d}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from scratch&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">param_prior_samples_numpyro</span><span class="p">[</span><span class="s1">&#39;V&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;from numpyro&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Parameter value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior for $V$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>(From this point forward, feel free to use either your implementation of the log density and prior sampler or the Numpyro implementation.)</p>
<p>Now, collect 1000 <em>prior</em> samples of <span class="math notranslate nohighlight">\(\theta=e^\xi\)</span>.
Plot the the following over the time interval <span class="math notranslate nohighlight">\([0, 12]\)</span> hours:</p>
<ul class="simple">
<li><p>The 95% credible interval for <span class="math notranslate nohighlight">\(c_1(t; \theta, m_\text{dose})\)</span></p></li>
<li><p>The 95% predictive interval for <span class="math notranslate nohighlight">\(c_1(t; \theta, m_\text{dose}) + \epsilon; ~ \epsilon \sim \mathcal{N}(0, \sigma)\)</span></p></li>
<li><p>A few samples of <span class="math notranslate nohighlight">\(c_1(t; \theta, m_\text{dose})\)</span></p></li>
</ul>
<p>Hint: You may simply run the following code cell to create the plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">t_plt</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">xi_prior_samples</span> <span class="o">=</span> <span class="n">sample_prior_xi</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1_000</span><span class="p">)</span>
<span class="n">theta_prior_samples</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">constrain</span><span class="p">)(</span><span class="n">xi_prior_samples</span><span class="p">)</span>
<span class="n">c1_prior_samples</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">solve_pk_iv_bolus</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))(</span><span class="n">theta_prior_samples</span><span class="p">,</span> <span class="n">t_plt</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">y_prior_predictive_samples</span> <span class="o">=</span> <span class="n">c1_prior_samples</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">jr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">c1_prior_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">q05_epistemic</span><span class="p">,</span> <span class="n">q95_epistemic</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">c1_prior_samples</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">q05_aleatoric</span><span class="p">,</span> <span class="n">q95_aleatoric</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">y_prior_predictive_samples</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_plt</span><span class="p">,</span> <span class="n">c1_prior_samples</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_plt</span><span class="p">,</span> <span class="n">q95_aleatoric</span><span class="p">,</span> <span class="n">q95_epistemic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_plt</span><span class="p">,</span> <span class="n">q95_epistemic</span><span class="p">,</span> <span class="n">q05_epistemic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Epistemic uncertainty&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_plt</span><span class="p">,</span> <span class="n">q05_epistemic</span><span class="p">,</span> <span class="n">q05_epistemic</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Aleatoric uncertainty&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Time (hr)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Concentration (g/L)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Prior samples&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-c-find-the-maximum-a-posteriori-estimate">
<h2>Part C - Find the maximum a-posteriori estimate<a class="headerlink" href="#part-c-find-the-maximum-a-posteriori-estimate" title="Link to this heading">#</a></h2>
<p>Now, we want to find the maximum a-posteriori (MAP) estimate <span class="math notranslate nohighlight">\(\xi^*\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\xi^* = \arg \max_\xi p(\xi|\mathbf{t}, \mathbf{y} , \sigma).
\]</div>
<p>We now have an unconstrained optimization problem.</p>
<p><strong>Do the following:</strong></p>
<ul class="simple">
<li><p>Use ADAM to find <span class="math notranslate nohighlight">\(\xi^*\)</span>. Show that the loss converges. You may want to run ADAM for a few different starting points to ensure you have found the global maximum.</p></li>
<li><p>Report the MAP estimate of the parameters <em>in constrained space</em>, i.e., <span class="math notranslate nohighlight">\(\theta^*=e^{\xi^*}\)</span>.</p></li>
<li><p>Plot the MAP estimate for the central compartment concentration <span class="math notranslate nohighlight">\(c_1(t; \theta^*, m_\text{dose})\)</span> <em>and</em> the 95% predictive interval (aleatoric uncertainty). (Hint: See part B.)
Overlay the observations on the plot.</p></li>
<li><p>Compute and report the <em>area under the curve</em> (AUC) of the <span class="math notranslate nohighlight">\(c_1\)</span>-<span class="math notranslate nohighlight">\(t\)</span> curve from <span class="math notranslate nohighlight">\(t=0\)</span> to <span class="math notranslate nohighlight">\(t=36\)</span> hours, i.e.,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\int_0^{36} c_1(t; \theta^*, m_\text{dose}) dt.
\]</div>
<p>   (AUC is a measure of the <em>total drug exposure</em>, and it helps assess drug efficacy and safety.)</p>
<p><strong>Your answer here:</strong> <br><br><br><br><br></p>
</section>
<section id="part-d-sample-the-posterior-with-mcmc">
<h2>Part D - Sample the posterior with MCMC<a class="headerlink" href="#part-d-sample-the-posterior-with-mcmc" title="Link to this heading">#</a></h2>
<p>Instead of only finding a point estimate of the parameters (as in Part C), we will characterize the full posterior distribution <span class="math notranslate nohighlight">\(p(\xi|\mathbf{t}, \mathbf{y}, \sigma)\)</span>.
This will allow us to quantify our <em>epistemic</em> or <em>lack-of-data</em> uncertainty about the parameters.</p>
<p><strong>Do the following:</strong></p>
<ul class="simple">
<li><p>Sample the from posterior distribution <span class="math notranslate nohighlight">\(p(\xi|\mathbf{t}, \mathbf{y}, \sigma)\)</span> using the No-U-Turn Sampler (NUTS). Use at least 3 chains and 1000 samples per chain. (Hint: See <a class="reference external" href="https://predictivesciencelab.github.io/advanced-scientific-machine-learning/inverse/sampling/04_nuts_blackjax.html">this hands-on activity demonstrating NUTS in blackjax</a>.)</p></li>
<li><p>Show the trace plots of the MCMC chains. Report MCMC diagnostics (R-hat, ESS). Argue whether the chains have converged. (Hint: Use <code class="docutils literal notranslate"><span class="pre">arviz.plot_trace</span></code> and <code class="docutils literal notranslate"><span class="pre">arviz.summary</span></code>.)</p></li>
<li><p>Plot all the <span class="math notranslate nohighlight">\(\xi\)</span> samples onto a scatterplot matrix. (Hint: Use <code class="docutils literal notranslate"><span class="pre">seaborn.pairplot</span></code>.)
Comment on how identifiable the parameters are.</p></li>
<li><p>Compute the concentration <span class="math notranslate nohighlight">\(c_1\)</span> for each posterior sample, over the time interval <span class="math notranslate nohighlight">\([0, 12]\)</span>.
Plot the 95% credible interval, 95% predictive interval, and a few samples from the posterior. (Hint: See Part B.)</p></li>
<li><p>Plot a histogram of the area under the curve (AUC) for the posterior samples (see part C for the definition of “AUC”).</p></li>
</ul>
<p><strong>Your answer here:</strong> <br><br><br><br><br></p>
</section>
<section id="part-e-variational-inference-with-mean-field-gaussian-guide">
<h2>Part E - Variational inference with mean-field Gaussian guide<a class="headerlink" href="#part-e-variational-inference-with-mean-field-gaussian-guide" title="Link to this heading">#</a></h2>
<p><strong>Do the following:</strong></p>
<ul class="simple">
<li><p>Construct a mean-field multivariate Gaussian guide (i.e., diagonal covariance matrix)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
q_\phi(\xi)=\mathcal{N}(\xi| \mu_\phi, \sigma_\phi I).
\]</div>
<ul class="simple">
<li><p>Maximize the Evidence Lower Bound (ELBO) with respect to the guide parameters <span class="math notranslate nohighlight">\(\phi\)</span> so that the guide approximates the posterior, i.e.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
q_\phi\approx p(\xi|\mathbf{t}, \mathbf{y}, \sigma).
\]</div>
<p>   (Hint: See <a class="reference external" href="https://predictivesciencelab.github.io/advanced-scientific-machine-learning/inverse/vi/02_catalysis.html">this hands-on activity implementing VI with a full-rank Gaussian guide</a>.
If you use <code class="docutils literal notranslate"><span class="pre">FullRankGaussianGuide</span></code> from the hands-on activity, at a minimum you will need to modify <code class="docutils literal notranslate"><span class="pre">Sigma</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, and <code class="docutils literal notranslate"><span class="pre">get_num_guide_params</span></code> to match the form of a mean-field Gaussian guide.
)</p>
<ul class="simple">
<li><p>Argue whether the optimization converged.</p></li>
<li><p>Collect 5,000 (approximate) posterior samples of <span class="math notranslate nohighlight">\(\xi\)</span> from the trained guide.</p></li>
<li><p>Plot all the <span class="math notranslate nohighlight">\(\xi\)</span> samples onto a scatterplot matrix.
Overlay the MCMC samples from part D.
Use transparency so that both VI and MCMC samples are visible.
How well does mean-field Gaussian VI approximate the posterior?</p></li>
<li><p>As in parts B and D, compute the concentration <span class="math notranslate nohighlight">\(c_1\)</span> for each posterior sample, over the time interval <span class="math notranslate nohighlight">\([0, 12]\)</span>.
Plot the 95% credible interval, 95% predictive interval, and a few samples from the posterior.</p></li>
<li><p>Plot a histogram of the AUCs of the posterior samples.
Overlay (with transparency) the AUC histogram from part D (MCMC).
Do they match?</p></li>
</ul>
<p><strong>Your answer here:</strong> <br><br><br><br><br></p>
</section>
<section id="part-f-variational-inference-with-full-rank-gaussian-guide">
<h2>Part F - Variational inference with full-rank Gaussian guide<a class="headerlink" href="#part-f-variational-inference-with-full-rank-gaussian-guide" title="Link to this heading">#</a></h2>
<p><strong>Do the following:</strong></p>
<ul class="simple">
<li><p>Construct a full-rank multivariate Gaussian guide</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
q_\phi(\xi) &amp;=\mathcal{N}(\xi| \mu_\phi, \Sigma_\phi) \\
\Sigma_\phi &amp;= L_\phi L_\phi^T \\
\end{align*}
\end{split}\]</div>
<p>   where <span class="math notranslate nohighlight">\(L_\phi\)</span> is a lower-triangular matrix parameterized by <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<ul class="simple">
<li><p>Repeat the steps from part E (including the plots) using the new guide.</p></li>
<li><p>How do the full-rank and mean-field VI approximations compare?</p></li>
</ul>
<p><strong>Your answer here:</strong> <br><br><br><br><br></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./homework"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05_homework.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Homework 5</p>
      </div>
    </a>
    <a class="right-next"
       href="07_homework.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 7 - TEMPLATE - DO NOT DO IT YET</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Homework 6</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#student-details">Student details</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-why-does-the-metropolis-algorithm-work">Problem 1 - Why does the Metropolis algorithm work</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-prove-that-detailed-balance-implies-stationarity">Part A - Prove that detailed balance implies stationarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-the-metropolis-hastings-transition-kernel">Part B - The Metropolis-Hastings transition kernel</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-i-derive-the-transition-kernel-of-the-metropolis-algorithm">B.I - Derive the transition kernel of the Metropolis algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-ii-show-that-the-transition-kernel-satisfies-the-detailed-balance-condition">B.II - Show that the transition kernel satisfies the detailed balance condition</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2-mathematics-of-variational-inference">Problem 2 - Mathematics of Variational Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-parameterization-of-a-covariance-matrix">Part A - Parameterization of a covariance matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-i-show-that-sigma-is-positive-semi-definite">A.I - Show that <span class="math notranslate nohighlight">\(\Sigma\)</span> is positive semi-definite.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-ii-numerical-exploration-of-a-rank-k-covariance-matrix">A.II - Numerical exploration of a rank-<span class="math notranslate nohighlight">\(k\)</span> covariance matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-iii-low-rank-approximation-that-is-actually-positive-definite">A.III - Low-rank approximation that is actually positive definite</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-multi-point-convexity">Part B - Multi-point convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-jensen-s-inequality">Part C - Jensen’s inequality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-non-negativity-of-the-kl-divergence">Part D - Non-negativity of the KL divergence</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-calibrating-a-pharmacokinetic-model">Problem 3 - Calibrating a pharmacokinetic model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-implement-the-unnormalized-log-posterior-density">Part A - Implement the (unnormalized) log posterior density</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-plot-the-prior-predictive-distribution">Part B - Plot the prior predictive distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-find-the-maximum-a-posteriori-estimate">Part C - Find the maximum a-posteriori estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-sample-the-posterior-with-mcmc">Part D - Sample the posterior with MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-variational-inference-with-mean-field-gaussian-guide">Part E - Variational inference with mean-field Gaussian guide</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-f-variational-inference-with-full-rank-gaussian-guide">Part F - Variational inference with full-rank Gaussian guide</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ilias Bilionis
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright Copyright © 2025 Ilias Bilionis/Purdue University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
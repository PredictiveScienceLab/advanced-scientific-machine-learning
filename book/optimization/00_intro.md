# Optimization for Scientific Machine Learning

## Objectives

*Write me.*

+ Optimization problems, local minima, saddle points.
+ Gradient descent.
+ Newton's method.
+ L-BFGS.
+ Stochastic gradient descent.
+ Momentum.
+ AdaGrad.
+ RMSProp.
+ Adam.
+ AdamMax.
+ AdamW.
+ Optimizers in `Jax`: `optax`.
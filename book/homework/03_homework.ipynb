{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('png')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - TEMPLATE - DO NOT DO IT YET\n",
    "\n",
    "## References\n",
    "\n",
    "TBD\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "+ **Used generative AI to complete this assignment (Yes/No):**\n",
    "+ **Which generative AI tool did you use (if applicable)?:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - The Pythagorean theorem on Hilbert Spaces\n",
    "\n",
    "Let $H$ be a Hilbert space with inner product $\\langle \\cdot, \\cdot \\rangle$ and norm $\\| \\cdot \\|$. Let $x, y \\in H$. \n",
    "\n",
    "## Part A\n",
    "Prove that if $x$ and $y$ are orthogonal, then the Pythagorean theorem holds, i.e., \n",
    "\n",
    "$$\n",
    "\\| x + y \\|^2 = \\| x \\|^2 + \\| y \\|^2.\n",
    "$$\n",
    "\n",
    "*Hint:* Use the fact that $\\| x + y \\|^2 = \\langle x + y, x + y \\rangle$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Prove the following generalization of the Pythagorean theorem.\n",
    "Let $x_1,x_2,\\dots,x_n \\in H$ be pairwise orthogonal, i.e., $\\langle x_i, x_j \\rangle = 0$ for all $i \\neq j$. Then, \n",
    "\n",
    "$$\n",
    "\\| x_1 + x_2 + \\dots + x_n \\|^2 = \\| x_1 \\|^2 + \\| x_2 \\|^2 + \\dots + \\| x_n \\|^2.\n",
    "$$\n",
    "\n",
    "*Hint:* Use induction and the result from Part A.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - All infinite dimensional Hilbert spaces are isomorphic to $\\ell^2$\n",
    "\n",
    "We mentioned in the lecture that an infinite dimensional Hilbert space $H$ are isomorphic to $\\ell^2$. In this problem we will prove this result.\n",
    "Intuitively, this means that we can think of vectors in $H$ as infinite dimensional vectors in $\\ell^2$.\n",
    "It is as if the space $H$ is a relabeling of the space $\\ell^2$.\n",
    "First, recall that\n",
    "\n",
    "$$\n",
    "\\ell^2 = \\left\\{ a = (a_1, a_2, \\dots) \\mid \\sum_{i=1}^\\infty |a_i|^2 < \\infty \\right\\}.\n",
    "$$\n",
    "\n",
    "To show that two spaces are isomorphic, we need to show that there exists a bijective linear map between them which keeps the inner product intact.\n",
    "Bijection means that the map is one-to-one and onto.\n",
    "So, we need to find an invertible, linear map:\n",
    "\n",
    "$$\n",
    "T: H \\to \\ell^2.\n",
    "$$\n",
    "\n",
    "To keep the inner product intact, we need to show that for all $x, y \\in H$,\n",
    "\n",
    "$$\n",
    "\\langle x, y \\rangle = \\langle T(x), T(y) \\rangle_{\\ell^2}.\n",
    "$$\n",
    "\n",
    "Here, on the left we have the inner product in $H$ and on the right we have the inner product in $\\ell^2$.\n",
    "If the inner products are intact, orthogonality is preserved by $T$.\n",
    "And also norms are preserved, since $\\| x \\| = \\sqrt{\\langle x, x \\rangle}$.\n",
    "\n",
    "Okay, this is what you will have to do.\n",
    "I will give you the right $T$ and you will have to show that it is linear, invertible, and keeps the inner product intact.\n",
    "\n",
    "Recall that since $H$ is separable, it has a countable orthonormal basis $\\{ e_1, e_2, \\dots \\}$.\n",
    "This means that every vector $x \\in H$ can be written as\n",
    "\n",
    "$$\n",
    "    x = \\sum_{i=1}^\\infty \\langle x, e_i \\rangle e_i.\n",
    "$$\n",
    "\n",
    "The idea is to use the Fourier coefficients $\\langle x, e_i \\rangle$ as the entries of the vector $T(x)$:\n",
    "\n",
    "$$\n",
    "T(x) = ( \\langle x, e_1 \\rangle, \\langle x, e_2 \\rangle, \\dots ).\n",
    "$$\n",
    "\n",
    "## Part A\n",
    "\n",
    "Show that $T(x)$ is indeed in $\\ell^2$ for all $x \\in H$. That is, show that $\\sum_{i=1}^\\infty |\\langle x, e_i \\rangle|^2 < \\infty$.\n",
    "\n",
    "*Hint:* Use Parseval's identity.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Show that $T$ is a linear map, i.e., show that for all $x, y \\in H$ and $\\alpha, \\beta \\in \\mathbb{R}$,\n",
    "\n",
    "$$\n",
    "T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y).\n",
    "$$\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Show that $T$ is onto. \n",
    "\n",
    "*Hint:* Take a vector $a \\in \\ell^2$ and show that there exists a vector $x \\in H$ such that $T(x) = a$. Just try to write down the vector $x$ in terms of $a$ and the orthonormal basis $\\{ e_1, e_2, \\dots \\}$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "Show that $T$ is one-to-one.\n",
    "\n",
    "*Hint:* Take two vectors $x, y \\in H$ and show that if $T(x) = T(y)$, then $x = y$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E\n",
    "\n",
    "Show that $T$ keeps the inner product intact. That is, show that for all $x, y \\in H$,\n",
    "\n",
    "$$\n",
    "\\langle x, y \\rangle = \\langle T(x), T(y) \\rangle_{\\ell^2}.\n",
    "$$\n",
    "\n",
    "*Hint:* Use the fact that $T$ is linear and the definition of $T$. The inner product of two vectors in $\\ell^2$ is defined as $\\langle a, b \\rangle_{\\ell^2} = \\sum_{i=1}^\\infty a_i b_i$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Numerical Construction of Polynomial Chaos\n",
    "\n",
    "Through this problem, you are going to construct orthogonal polynomials for the exponential distribution and test a few things with them.\n",
    "You need to familiarize yourself with [this hands-on-activity](https://predictivesciencelab.github.io/advanced-scientific-machine-learning/polynomial_chaos/04_orthpol_demo.html) before you proceed.\n",
    "\n",
    "## Part A\n",
    "\n",
    "Consider the random variable:\n",
    "\n",
    "$$\n",
    "\\Xi \\sim \\exp(1).\n",
    "$$\n",
    "\n",
    "The exponential distribution has the following probability density function:\n",
    "\n",
    "$$\n",
    "f_\\Xi(\\xi) = \\begin{cases}\n",
    "e^{-\\xi} & \\xi \\geq 0 \\\\\n",
    "0 & \\xi < 0\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Use the `orthojax` package to construct the first 5 orthogonal polynomials for $\\Xi$.\n",
    "Plot them on the same figure for $\\xi \\in [0, 5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Hint: You can use the function orthojax.make_orthogonal_polynomial\n",
    "# but you need to pass the argument right=jnp.inf to indicate that\n",
    "# the right endpoint is infinity.\n",
    "\n",
    "import orthojax as ojax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Project the function:\n",
    "\n",
    "$$\n",
    "f(\\xi) = \\sin(x)\n",
    "$$\n",
    "\n",
    "onto the first 5 orthogonal polynomials for $\\Xi$. Plot the function $f$ and its projection on the same figure for $\\xi \\in [0, 5]$.\n",
    "\n",
    "*Hint:* Do exactly what I do in the activity. You need to extract from `poly` the quadrature rule so that you can do the inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Use the polynomial projection to calculate the mean and variance of the random variable\n",
    "\n",
    "$$\n",
    "Y = f(\\Xi) = \\sin(\\Xi).\n",
    "$$\n",
    "\n",
    "Compare to Monte Carlo estimates or the exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 - Uncertainty Propagation with Polynomial Chaos\n",
    "\n",
    "Consider the Lorenz system:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dot{x} &= \\sigma(y-x),\\\\\n",
    "\\dot{y} &= x(\\rho-z)-y,\\\\\n",
    "\\dot{z} &= xy-\\beta z,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with parameters $\\sigma=10$, $\\beta=8/3$, and $\\rho=28$.\n",
    "Take the initial conditions to be random:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x(0) &\\sim \\mathcal{N}(0, 0.01),\\\\\n",
    "y(0) &\\sim \\mathcal{N}(0, 0.01),\\\\\n",
    "z(0) &\\sim \\mathcal{N}(0, 0.01).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Part A - Build a Polynomial Chaos Surrogate\n",
    "\n",
    "Build a polynomial chaos surrogate. Calculate the mean and the variance as a function of time. Compare the result to Monte Carlo estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Predictions\n",
    "\n",
    "Generate three random initial conditions and propagate them forward in time using the surrogate. Plot only $x$ as a function of time for each initial condition. Compare to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Probability Density Function\n",
    "\n",
    "Use your surrogate to estimate the probability density function of $x$ at $t=1, 2, 5,$ and $10$.\n",
    "Use different plots for each case.\n",
    "You can do this, by generating $100,000$ initial conditions, propagating them forward through the surrogate and then plotting a histogram of the results.\n",
    "Compare to Monte Carlo PDFs. Use transparency in your plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I may add one more homework problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Physics-informed Neural Networks for Solving a Neo-Hookean Hyperelasticity Problem\n",
    "\n",
    "*The original version of this problem was developed by Atharva Hans as a companion to [this](https://youtu.be/o9JaZGWekWQ).\n",
    "\n",
    "Consider a neo-Hookean square body defined on $(x,y) \\in [0,1]^2$. Let $\\mathbf{u}(x,y) = (u_1, u_2)$ describe the displacement field for this body.\n",
    "This body is subjected to the following displacement boundary conditions:\n",
    "\n",
    "$$\n",
    "u_1(0,y) = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_2(0,y) = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_1(1,y) = \\delta,\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_2(1,y) = 0,\n",
    "$$\n",
    "\n",
    "with $\\delta$ referring to the applied displacement along the x-direction.\n",
    "\n",
    "For this hyperelastic material, the stored energy $E_b$ in the body can be expressed in as:\n",
    "\n",
    "$$\n",
    "E_b[\\mathbf{u}(\\cdot)] = \\int_{[0,1]^2}\\left\\{\\frac{1}{2}(\\sum_{i=1}^2\\sum_{j=1}^2{F_{ij}^2} - 2)- \\ln(\\det(\\mathbf{F})) + 50\\ln(\\det(\\mathbf{F}))^2\\right\\} dxdy,\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mathbf{F} = \\mathbf{I} + \\nabla \\mathbf{u},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is an identity matrix.\n",
    "\n",
    "The final orientation of this body is described by a displacement field that minimizes the stored energy $E_b$.\n",
    "The idea is to use a neural network to approximate the displacement field and train it by minimizing the stored energy $E_b$.\n",
    "\n",
    "To automatically satisfy the boundary conditions, we will use this approximation:\n",
    "$$\n",
    "u_1(x,y) = \\delta - \\delta(1-x) + x(1-x)N_1(x,y;\\theta),\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "u_2(x,y) = x(1-x)N_2(x,y;\\theta)\n",
    "$$\n",
    "where $N_1(x,y;\\theta)$ and $N_2(x,y;\\theta)$ are neural networks.\n",
    "\n",
    "## Part A\n",
    "\n",
    "Solve the problem above for $\\delta=0.1$ using a physics-informed neural network (PINN).\n",
    "Use separate neural networks for $N_1(x,y;\\theta)$ and $N_2(x,y;\\theta)$.\n",
    "Start with a multi-layer perceptron with 3 hidden layers, each with 128 units, and tanh activations.\n",
    "Add a Fourier feature layer at the beginning of the network.\n",
    "Feel free to change the architecture if you think it is necessary.\n",
    "\n",
    "Use `equinox` for the neural networks and `optax` for the optimization.\n",
    "Use a sampling average of 32 collocation points to compute the integral of the stored energy.\n",
    "Use the Adam optimizer with a learning rate of 0.001 for 1000 iterations to debug.\n",
    "Feel free to play with the learning rate, the number of collocation points, and the number of iterations.\n",
    "\n",
    "Show the evolution of the loss function over the iterations.\n",
    "Plot the final displacement field (plot $u_1(x,y)$ and $u_2(x,y)$ separately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here. Use as many markdown and code blocks as you want.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Solve the problem for $\\delta=0.5$ using the same architecture as above.\n",
    "It will likely fail to train.\n",
    "If yes, then use the solution of $\\delta=0.1$ as the initial guess for $\\delta=0.2$, and then use the solution of $\\delta=0.2$ as the initial guess for $\\delta=0.3$, and so on, until you reach $\\delta=0.5$.\n",
    "This is called transfer learning.\n",
    "\n",
    "At the end, plot the final displacement field for $\\delta=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here. Use as many markdown and code blocks as you want.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Solve the parametric problem for $\\delta \\in [0,0.5]$. That is, build a neural network that takes $\\delta$ as input and outputs the displacement field. To do this:\n",
    "+ Modify the loss function to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\int_0^{0.5} \\int_{[0,1]^2} \\left\\{\\frac{1}{2}(\\sum_{i}\\sum_{j}{F_{ij}^2} - 2)- \\ln(\\det(\\mathbf{F})) + 50\\ln(\\det(\\mathbf{F}))^2\\right\\} dxdy d\\delta.\n",
    "$$\n",
    "\n",
    "+ Modify the neural networks to take $\\delta$ as input, say $N_1(x,y;\\delta;\\theta)$ and $N_2(x,y;\\delta;\\theta)$. Your field will be $\\mathbf{u}(x,y;\\delta;\\theta)$.\n",
    "Use the following architecture for the neural networks:\n",
    "\n",
    "$$\n",
    "N_1(x,y;\\delta) = \\sum_{i=1}^n b_{1,i}(\\delta)t_{1,i}(x,y).\n",
    "$$\n",
    "\n",
    "Here, $n$ is your choice (start with $n=10$), $b_{1,i}$ is a neural network that takes $\\delta$ as input and outputs a scalar, and $t_{1,i}(x,y)$ is a multi-layer perceptron with 3 hidden layers, each with 128 units, and tanh activations, and Fourier features at the beginning. The same applies to $N_2(x,y;\\delta)$. This representation resembles an expansion in terms of basis functions.\n",
    "The same architecture appears in DeepONet.\n",
    "\n",
    "Plot the $x$ and $y$ displacement at $x=0.5, y=0.5$ as a function of $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

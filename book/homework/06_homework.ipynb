{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('png')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - TEMPLATE - DO NOT DO IT YET\n",
    "\n",
    "## References\n",
    "\n",
    "+ Module 5: Inverse problems in deterministic scientifc models\n",
    "   - Inverse problems basics\n",
    "   - Sampling from posteriors\n",
    "   - Variational inference\n",
    "   - Deterministic, finite-dimensional dynamical systems\n",
    "   - PDE-constrained inverse problems\n",
    "   - Purely data-driven learning of dynamical systems\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "+ **Used generative AI to complete this assignment (Yes/No):**\n",
    "+ **Which generative AI tool did you use (if applicable)?:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Why does the Metropolis algorithm work\n",
    "\n",
    "The objective of this problem is to understand why the Metropolis algorithm works.\n",
    "\n",
    "Consider a Markov chain $x_n$ with transition probabilities $p(x_{n+1}|x_n)$ and a probability density $\\pi(x)$.\n",
    "We say that $x_n$ has stationary distribution $\\pi$ if:\n",
    "\n",
    "$$\n",
    "\\pi(x_{n+1}) = \\int p(x_{n+1}|x_n)\\pi(x_n)dx_n.\n",
    "$$\n",
    "\n",
    "Intuitively, we can think of the equation above as follows.\n",
    "If we, somehow, sample $x_n$ from $\\pi$ and then sample $x_{n+1}$ from the transition probability $p(x_{n+1}|x_n)$, then $x_{n+1}$ is also a sample from $\\pi(x)$.\n",
    "It is like once we have a sample $\\pi$ sampling the Markov chain keeps giving us samples from $\\pi$.\n",
    "\n",
    "We say that the Markov chain $x_n$ is *reversible* with respect to $\\pi$ (equivalently, satisfies the *detailed balance* condition) with respect to $\\pi$, if:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n)\\pi(x_n) = p(x_n|x_{n+1})\\pi(x_{n+1}).\n",
    "$$\n",
    "\n",
    "Intuitively, this condition means that going from sampling $x_{n}$ from $\\pi$ and transition to $x_{n+1}$ has the same probability as doing the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Prove that detailed balance implies stationarity\n",
    "\n",
    "Suppose that the Markov chain $x_n$ satisfies the detailed balance condition with respect to $\\pi$. Prove that $\\pi$ is a stationary distribution of the Markov chain.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - The Metropolis-Hastings transition kernel\n",
    "\n",
    "Let $\\pi(x)$ be the target distribution.\n",
    "Let $q(\\tilde{x}_{n+1}|x_n)$ be a proposal distribution of the Metropolis-Hastings algorithm.\n",
    "\n",
    "The Metropolis-Hastings algorithm results in a Markov chain $x_n$ defined as follows:\n",
    "\n",
    "+ Sample $\\tilde{x}_{n+1} \\sim q(\\tilde{x}_{n+1}|x_n)$\n",
    "+ Accept $\\tilde{x}_{n+1}$ and set $x_{n+1} = \\tilde{x}_{n+1}$ with probability $\\alpha(x_n, \\tilde{x}_{n+1})$\n",
    "+ Reject $\\tilde{x}_{n+1}$ and set $x_{n+1} = x_n$ with probability $1-\\alpha(x_n, \\tilde{x}_{n+1}),$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\alpha(x_n, \\tilde{x}_{n+1}) = \\min\\left(1, \\frac{\\pi(\\tilde{x}_{n+1})q(x_n|\\tilde{x}_{n+1})}{\\pi(x_n)q(\\tilde{x}_{n+1}|x_n)}\\right).\n",
    "$$\n",
    "\n",
    "The purpose of this problem is to show that the transition kernel of the resulting Markov chain satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
    "\n",
    "### B.I - Derive the transition kernel of the Metropolis algorithm\n",
    "\n",
    "Show that the transition kernel of the Metropolis algorithm is:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) = \\alpha(x_n, x_{n+1})q(x_{n+1}|x_n) +\n",
    "\\delta(x_{n+1} - x_n)\\int (1 - \\alpha(x_n, \\tilde{x}_{n+1}))q(\\tilde{x}_{n+1}|x_n)d\\tilde{x}_{n+1},\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the Dirac delta function.\n",
    "\n",
    "Hints:\n",
    "\n",
    "+ Introduce an intermediate variable $i$ that takes the value $1$ if the proposed move is accepted and $0$ otherwise. That is:\n",
    "\n",
    "$$\n",
    "i | x_n, \\tilde{x}_{n+1} \\sim \\begin{cases}\n",
    "    1 & \\text{with probability } \\alpha(x_n, \\tilde{x}_{n+1}) \\\\\n",
    "    0 & \\text{with probability } 1 - \\alpha(x_n, \\tilde{x}_{n+1}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "+ Write the joint distribution $p(x_{n+1}| i, x_n, \\tilde{x}_{n+1})$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) = [\\delta(x_{n+1} - \\tilde{x}_{n+1})]^i [\\delta(x_{n+1} - x_n)]^{1-i}.\n",
    "$$\n",
    "\n",
    "+ Use the sum rule to express $p(x_{n+1}|x_n)$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) = \\int \\sum_i p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) p(i | x_n, \\tilde{x}_{n+1}) q(\\tilde{x}_{n+1}|x_n) d\\tilde{x}_{n+1}.\n",
    "$$\n",
    "\n",
    "+ Use the definition of the Dirac delta function to simplify the expression.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.II - Show that the transition kernel satisfies the detailed balance condition\n",
    "\n",
    "Show that the transition kernel of the Metropolis algorithm satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
    "Mathematically, you need to show that:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) \\pi(x_n) = p(x_n|x_{n+1}) \\pi(x_{n+1}).\n",
    "$$\n",
    "\n",
    "Hints:\n",
    "\n",
    "+ First prove that $a(x_n, x_{n+1})q(x_{n+1}|x_n)\\pi(x_n) = a(x_{n+1}, x_n)q(x_n|x_{n+1})\\pi(x_{n+1})$.\n",
    "+ Then, reuse the result above the symmetry of the Dirac delta function.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Mathematics of Variational Inference\n",
    "\n",
    "## Part A - Parameterization of a covariance matrix\n",
    "\n",
    "The purpose is to show that the commonly used rank-$k$ parameterization of the covariance matrix is indeed positive definite.\n",
    "\n",
    "Let $k$ be a positive integer, and $\\lambda_1, \\dots, \\lambda_k$ be real numbers.\n",
    "Let $d$ be another positive integer (the dimension of the covariance matrix) with $d \\geq k$.\n",
    "Let $u_1, \\dots, u_k$ be $d$-dimensional vectors, not necessarily orthogonal, but linearly independent.\n",
    "\n",
    "Consider the following matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_{i=1}^k e^{\\lambda_i} u_i u_i^\\top.\n",
    "$$\n",
    "\n",
    "### A.I - Show that $\\Sigma$ is positive semi-definite.\n",
    "\n",
    "Hint: You need to show that for any non-zero vector $x \\in \\mathbb{R}^d$, the quadratic form $x^\\top \\Sigma x \\geq 0$.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.II - Numerical exploration of a rank-$k$ covariance matrix\n",
    "\n",
    "Set $d=100$ and $k=10$.\n",
    "Randomly generate $u_1, \\dots, u_k$ from the standard normal distribution.\n",
    "Randomly generate $\\lambda_1, \\dots, \\lambda_k$ from the standard normal distribution.\n",
    "Write Jax code (without a loop) to form the matrix $\\Sigma$ as defined above.\n",
    "Generate a random $\\Sigma$ and plot the eigenvalues.\n",
    "Are they all non-negative?\n",
    "What is the determinant of $\\Sigma$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.III - Low-rank approximation that is actually positive definite\n",
    "\n",
    "In the previous part, we saw that the rank-$k$ approximation is not positive definite.\n",
    "To fix it, we typically use this parameterization instead:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_{i=1}^k \\lambda_i u_i u_i^\\top + \\text{diag}(e^{\\theta_1}, \\dots, e^{\\theta_d}),\n",
    "$$\n",
    "\n",
    "where $\\theta_1, \\dots, \\theta_d$ are real numbers.\n",
    "\n",
    "Modify your Jax code and generate a random $\\Sigma$ using this parameterization.\n",
    "Plot the eigenvalues.\n",
    "Are they all non-negative?\n",
    "What is the determinant of $\\Sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Multi-point convexity\n",
    "\n",
    "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function.\n",
    "Let $x_1, \\dots, x_n \\in \\mathbb{R}^d$ be $n$ points.\n",
    "Let $w_1, \\dots, w_n \\in \\mathbb{R}$ be $n$ weights.\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n w_i x_i\\right) \\leq \\sum_{i=1}^n w_i f(x_i).\n",
    "$$\n",
    "\n",
    "Hint: Use the definition of convexity and induction.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Jensen's inequality\n",
    "\n",
    "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function that is continuous.\n",
    "Let $X$ be a random variable with values in $\\mathbb{R}^d$.\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$\n",
    "f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n",
    "$$\n",
    "\n",
    "Hint: Use Part B and the law of large numbers.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Non-negativity of the KL divergence\n",
    "\n",
    "Let $p$ and $q$ be two probability distributions on $\\mathbb{R}^d$.\n",
    "Show that the KL divergence $D_{KL}(p\\|q)$ is always non-negative.\n",
    "\n",
    "Hint: Use the fact that $-\\log$ is a convex function and Jensen's inequality.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Partially Observed Lorenz System\n",
    "\n",
    "Below, I am going to generate some data from the Lorenz system.\n",
    "You are going to pretend that you only observe the $x$ component and try to identify dynamics from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0 / 3.0\n",
    "dt = 0.01\n",
    "num_steps = int(20.0 / dt)\n",
    "ts = np.linspace(0, 100, num_steps)\n",
    "x0 = np.array([-8.0, 7.0, 27.0])\n",
    "\n",
    "def vector_field(x, t):\n",
    "    return (\n",
    "        sigma * (x[1] - x[0]),\n",
    "        x[0] * (rho - x[2]) - x[1],\n",
    "        x[0] * x[1] - beta * x[2]\n",
    "    )\n",
    "xs = scipy.integrate.odeint(vector_field, x0, ts)\n",
    "\n",
    "# Find the exact derivatives - no noise\n",
    "from jax import vmap, jit\n",
    "vf = jit(vmap(vector_field, in_axes=(0, 0)))\n",
    "dxs = np.array(vf(xs, ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you should use are these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_xs = xs[:, 0]\n",
    "partial_dxs = dxs[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Applying SINDY on a partially observed system\n",
    "\n",
    "Try to apply SINDY on `partial_xs` and `partial_dxs`.\n",
    "Just try to express the right-hand-side of the dynamics using a high order polynomial.\n",
    "Do not use anything fancier as there is no way this can work.\n",
    "Demonstrate using some validation data that this doesn't work.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - The Hankel Matrix\n",
    "\n",
    "Part A failed because we tried to fit Markovian dynamics to a partially observed state.\n",
    "There are no Markovian dynamics for partially observed states.\n",
    "Partially observed states exhibit effective dynamics that appear to have memory (and noise).\n",
    "The Hankel matrix is a way to create variables that account for memory.\n",
    "We will try two variations.\n",
    "First, we will just try to learn dynamics directly on the columns of the Hankel matrix.\n",
    "This is not going to work if the memory we need is long.\n",
    "Then, we will use SVD to reduce the dimensionality of the Hankel matrix before attempting to learn the dynamics.\n",
    "\n",
    "Your data are $x(t_1),\\dots,x(t_m)$.\n",
    "The Hankel matrix is:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_\\ell = \\begin{bmatrix}\n",
    "x(t_1) & x(t_2) & x(t_3) & \\dots x(t_{m-\\ell})\\\\\n",
    "x(t_2) & x(t_3) & x(t_4) & \\dots x(t_{m-\\ell+1})\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\dots \\vdots\\\\\n",
    "x(t_\\ell) & x(t_{\\ell+1}) & x(t_{\\ell+3}) \\dots & x(t_m)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Write a function that forms the Hankel matrix given the data and $\\ell$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hankel(xs, ell):\n",
    "    \"\"\"Write a good docstring.\"\"\"\n",
    "    # write your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Apply SINDY on the Hankel matrix\n",
    "\n",
    "Form the Hankel matrices for $x(t)$ and $\\dot{x}(t)$ for $\\ell=5$.\n",
    "Try to represent the dynamics with a third degree polynomial.\n",
    "Validate your results.\n",
    "Do not expect this work very well.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Do SVD on the Hankel matrix\n",
    "\n",
    "Let's pick a big $\\ell$. Say $\\ell = 100$:\n",
    "+ Form the corresponding Hankel matrix and then do SVD on it.\n",
    "+ Plot the explained variance as a function of the number of singular values.\n",
    "+ How much variance do you explain with three dimensions (this is the intrinsic dimensionality of the dynamical system)?\n",
    "+ Visualize the first three POD modes.\n",
    "+ Project the Hankel matrix columns to three dimensions (POD amplitudes/principal components).\n",
    "+ Plot the time series of each one of the principal components.\n",
    "+ Plot the 3D trajectory of the principal components.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E - Find the time derivatives of the principal components of the Hankel matrix\n",
    "\n",
    "To do SINDY, we need to have time derivatives.\n",
    "So, you have to find the time derivatives of the principal components of the Hankel matrix.\n",
    "You have two options:\n",
    "+ Work out analytically how the observed `partial_dxs` will project on the POD modes, or;\n",
    "+ Use numerical differentiation to find the required time derivatives (Google around for the best Python library for numerical differentiation). In this case, simple finite differences should work.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F - Do SINDY on the principal components of the Hankel matrix\n",
    "\n",
    "You are now ready to do SINDY on the principal components of the Hankel matrix.\n",
    "Use a polynomial of degree 5 as the right-hand-side.\n",
    "Try to validate your results.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - SINDY with measurement noise and no derivatives\n",
    "\n",
    "Let's get back to the Lorenz system. This time, we are going to assume that we have access to the full state, but we do not have the derivative, and the measurements are corrupted by noise.\n",
    "So, your available data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "noisy_x = xs + eta * np.random.normal(0, 1.0, xs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the package [derivative](https://pypi.org/project/derivative/) (which part of the `pysindy` ecosystem) and:\n",
    "- Use a suitable method to estimate the derivative dx/dt from the noisy data `noisy_xs`.\n",
    "- Apply SINDY to the denoised data and the numerical derivatives.\n",
    "- Validate your results.\n",
    "\n",
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

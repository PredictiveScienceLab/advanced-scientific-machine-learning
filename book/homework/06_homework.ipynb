{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('png')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 - TEMPLATE - DO NOT DO IT YET\n",
    "\n",
    "## References\n",
    "\n",
    "+ Module 5: Inverse problems in deterministic scientifc models\n",
    "   - Inverse problems basics\n",
    "   - Sampling from posteriors\n",
    "   - Variational inference\n",
    "   - Deterministic, finite-dimensional dynamical systems\n",
    "   - PDE-constrained inverse problems\n",
    "   - Purely data-driven learning of dynamical systems\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**\n",
    "+ **Used generative AI to complete this assignment (Yes/No):**\n",
    "+ **Which generative AI tool did you use (if applicable)?:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Why does the Metropolis algorithm work\n",
    "\n",
    "The objective of this problem is to understand why the Metropolis algorithm works.\n",
    "\n",
    "Consider a Markov chain $x_n$ with transition probabilities $p(x_{n+1}|x_n)$ and a probability density $\\pi(x)$.\n",
    "We say that $x_n$ has stationary distribution $\\pi$ if:\n",
    "\n",
    "$$\n",
    "\\pi(x_{n+1}) = \\int p(x_{n+1}|x_n)\\pi(x_n)dx_n.\n",
    "$$\n",
    "\n",
    "Intuitively, we can think of the equation above as follows.\n",
    "If we, somehow, sample $x_n$ from $\\pi$ and then sample $x_{n+1}$ from the transition probability $p(x_{n+1}|x_n)$, then $x_{n+1}$ is also a sample from $\\pi(x)$.\n",
    "It is like once we have a sample $\\pi$ sampling the Markov chain keeps giving us samples from $\\pi$.\n",
    "\n",
    "We say that the Markov chain $x_n$ is *reversible* with respect to $\\pi$ (equivalently, satisfies the *detailed balance* condition) with respect to $\\pi$, if:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n)\\pi(x_n) = p(x_n|x_{n+1})\\pi(x_{n+1}).\n",
    "$$\n",
    "\n",
    "Intuitively, this condition means that going from sampling $x_{n}$ from $\\pi$ and transition to $x_{n+1}$ has the same probability as doing the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Prove that detailed balance implies stationarity\n",
    "\n",
    "Suppose that the Markov chain $x_n$ satisfies the detailed balance condition with respect to $\\pi$. Prove that $\\pi$ is a stationary distribution of the Markov chain.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - The Metropolis-Hastings transition kernel\n",
    "\n",
    "Let $\\pi(x)$ be the target distribution.\n",
    "Let $q(\\tilde{x}_{n+1}|x_n)$ be a proposal distribution of the Metropolis-Hastings algorithm.\n",
    "\n",
    "The Metropolis-Hastings algorithm results in a Markov chain $x_n$ defined as follows:\n",
    "\n",
    "+ Sample $\\tilde{x}_{n+1} \\sim q(\\tilde{x}_{n+1}|x_n)$\n",
    "+ Accept $\\tilde{x}_{n+1}$ and set $x_{n+1} = \\tilde{x}_{n+1}$ with probability $\\alpha(x_n, \\tilde{x}_{n+1})$\n",
    "+ Reject $\\tilde{x}_{n+1}$ and set $x_{n+1} = x_n$ with probability $1-\\alpha(x_n, \\tilde{x}_{n+1}),$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\alpha(x_n, \\tilde{x}_{n+1}) = \\min\\left(1, \\frac{\\pi(\\tilde{x}_{n+1})q(x_n|\\tilde{x}_{n+1})}{\\pi(x_n)q(\\tilde{x}_{n+1}|x_n)}\\right).\n",
    "$$\n",
    "\n",
    "The purpose of this problem is to show that the transition kernel of the resulting Markov chain satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
    "\n",
    "### B.I - Derive the transition kernel of the Metropolis algorithm\n",
    "\n",
    "Show that the transition kernel of the Metropolis algorithm is:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) = \\alpha(x_n, x_{n+1})q(x_{n+1}|x_n) +\n",
    "\\delta(x_{n+1} - x_n)\\int (1 - \\alpha(x_n, \\tilde{x}_{n+1}))q(\\tilde{x}_{n+1}|x_n)d\\tilde{x}_{n+1},\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the Dirac delta function.\n",
    "\n",
    "Hints:\n",
    "\n",
    "+ Introduce an intermediate variable $i$ that takes the value $1$ if the proposed move is accepted and $0$ otherwise. That is:\n",
    "\n",
    "$$\n",
    "i | x_n, \\tilde{x}_{n+1} \\sim \\begin{cases}\n",
    "    1 & \\text{with probability } \\alpha(x_n, \\tilde{x}_{n+1}) \\\\\n",
    "    0 & \\text{with probability } 1 - \\alpha(x_n, \\tilde{x}_{n+1}).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "+ Write the joint distribution $p(x_{n+1}| i, x_n, \\tilde{x}_{n+1})$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) = [\\delta(x_{n+1} - \\tilde{x}_{n+1})]^i [\\delta(x_{n+1} - x_n)]^{1-i}.\n",
    "$$\n",
    "\n",
    "+ Use the sum rule to express $p(x_{n+1}|x_n)$ in terms of $i$ and $\\tilde{x}_{n+1}$:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) = \\int \\sum_i p(x_{n+1}| i, x_n, \\tilde{x}_{n+1}) p(i | x_n, \\tilde{x}_{n+1}) q(\\tilde{x}_{n+1}|x_n) d\\tilde{x}_{n+1}.\n",
    "$$\n",
    "\n",
    "+ Use the definition of the Dirac delta function to simplify the expression.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.II - Show that the transition kernel satisfies the detailed balance condition\n",
    "\n",
    "Show that the transition kernel of the Metropolis algorithm satisfies the detailed balance condition with respect to $\\pi$, and thus $\\pi$ is its stationary distribution.\n",
    "Mathematically, you need to show that:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_n) \\pi(x_n) = p(x_n|x_{n+1}) \\pi(x_{n+1}).\n",
    "$$\n",
    "\n",
    "Hints:\n",
    "\n",
    "+ First prove that $a(x_n, x_{n+1})q(x_{n+1}|x_n)\\pi(x_n) = a(x_{n+1}, x_n)q(x_n|x_{n+1})\\pi(x_{n+1})$.\n",
    "+ Then, reuse the result above the symmetry of the Dirac delta function.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Mathematics of Variational Inference\n",
    "\n",
    "## Part A - Parameterization of a covariance matrix\n",
    "\n",
    "The purpose is to show that the commonly used rank-$k$ parameterization of the covariance matrix is indeed positive definite.\n",
    "\n",
    "Let $k$ be a positive integer, and $\\lambda_1, \\dots, \\lambda_k$ be real numbers.\n",
    "Let $d$ be another positive integer (the dimension of the covariance matrix) with $d \\geq k$.\n",
    "Let $u_1, \\dots, u_k$ be $d$-dimensional vectors, not necessarily orthogonal, but linearly independent.\n",
    "\n",
    "Consider the following matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_{i=1}^k e^{\\lambda_i} u_i u_i^\\top.\n",
    "$$\n",
    "\n",
    "### A.I - Show that $\\Sigma$ is positive semi-definite.\n",
    "\n",
    "Hint: You need to show that for any non-zero vector $x \\in \\mathbb{R}^d$, the quadratic form $x^\\top \\Sigma x \\geq 0$.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.II - Numerical exploration of a rank-$k$ covariance matrix\n",
    "\n",
    "Set $d=100$ and $k=10$.\n",
    "Randomly generate $u_1, \\dots, u_k$ from the standard normal distribution.\n",
    "Randomly generate $\\lambda_1, \\dots, \\lambda_k$ from the standard normal distribution.\n",
    "Write Jax code (without a loop) to form the matrix $\\Sigma$ as defined above.\n",
    "Generate a random $\\Sigma$ and plot the eigenvalues.\n",
    "Are they all non-negative?\n",
    "What is the determinant of $\\Sigma$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.III - Low-rank approximation that is actually positive definite\n",
    "\n",
    "In the previous part, we saw that the rank-$k$ approximation is not positive definite.\n",
    "To fix it, we typically use this parameterization instead:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\sum_{i=1}^k \\lambda_i u_i u_i^\\top + \\text{diag}(e^{\\theta_1}, \\dots, e^{\\theta_d}),\n",
    "$$\n",
    "\n",
    "where $\\theta_1, \\dots, \\theta_d$ are real numbers.\n",
    "\n",
    "Modify your Jax code and generate a random $\\Sigma$ using this parameterization.\n",
    "Plot the eigenvalues.\n",
    "Are they all non-negative?\n",
    "What is the determinant of $\\Sigma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as many code blocks and markdown blocks as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Multi-point convexity\n",
    "\n",
    "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function.\n",
    "Let $x_1, \\dots, x_n \\in \\mathbb{R}^d$ be $n$ points.\n",
    "Let $w_1, \\dots, w_n \\in \\mathbb{R}$ be $n$ weights.\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n w_i x_i\\right) \\leq \\sum_{i=1}^n w_i f(x_i).\n",
    "$$\n",
    "\n",
    "Hint: Use the definition of convexity and induction.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Jensen's inequality\n",
    "\n",
    "Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be a convex function that is continuous.\n",
    "Let $X$ be a random variable with values in $\\mathbb{R}^d$.\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$\n",
    "f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)].\n",
    "$$\n",
    "\n",
    "Hint: Use Part B and the law of large numbers.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Non-negativity of the KL divergence\n",
    "\n",
    "Let $p$ and $q$ be two probability distributions on $\\mathbb{R}^d$.\n",
    "Show that the KL divergence $D_{KL}(p\\|q)$ is always non-negative.\n",
    "\n",
    "Hint: Use the fact that $-\\log$ is a convex function and Jensen's inequality.\n",
    "\n",
    "**Answer:**\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Partially Observed Lorenz System\n",
    "\n",
    "Below, I am going to generate some data from the Lorenz system.\n",
    "You are going to pretend that you only observe the $x$ component and try to identify dynamics from that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "def convert_seconds_to_hms(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return int(hours), int(minutes), float(seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network on the GPU\n",
    "\n",
    "**NOTE:**\n",
    "For this notebook, you will need to have a GPU available. \n",
    "If your local computer does not have a [JAX-compatible compatible GPU](https://jax.readthedocs.io/en/latest/installation.html#supported-platforms), you can use the GPUs provided by Google Colab Pro. \n",
    "See [this guide](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/) on how to access GPUs on Google Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why GPUs?\n",
    "\n",
    "In non-trivial scientific machine learning tasks, we often need to use graphics processing units (GPUs) to accelerate computation. \n",
    "GPUs are a type of hardware that is optimized for parallel operations such as matrix multiplication, which is core to most machine learning algorithms.\n",
    "\n",
    "In this notebook, we'll compare GPU vs. CPU runtime for training a convolutional neural network (CNN) classifier on the MNIST dataset (i.e., the \"handwritten digits\" dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a CNN classifier for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_EPOCHS = 2\n",
    "MAX_STEPS = 100\n",
    "PRINT_EVERY = 10\n",
    "SEED = 5678\n",
    "\n",
    "import jax.random as jrandom\n",
    "key = jrandom.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset consists of 28-by-28 pixel, grayscale images of handwritten digits (0-9) that look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(images, labels):\n",
    "    ncols = len(labels)\n",
    "    nrows = int(np.ceil(len(images) / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))\n",
    "    for ax in axes.ravel():\n",
    "        ax.axis(\"off\")\n",
    "    for ax, image, label in zip(axes.ravel(), images, labels):\n",
    "        ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(label)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700.226152pt\" height=\"145.392052pt\" viewBox=\"0 0 700.226152 145.392052\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-09-23T13:23:14.535047</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 145.392052 \n",
       "L 700.226152 145.392052 \n",
       "L 700.226152 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p057c2b4809)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAKUAAAClCAYAAAA9Kz3aAAAFQklEQVR4nO3dzyt8exzH8fO9ZOHnxIJSEgtCUr4oiaQoUX4uZmFrNbGyUcqGlFhMLGSh/AdkQ8qPhRL5tVDKSsqSYoj8uHvzPvfe4x5fr8PzsXw1Hac8+9RphvnlOM6bAwj566tvAHiPKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCGHKCEn8bN/wMTERNw2NDTky7VPT0/NfWVlxdyfn5/NfWpqytxvbm4+dF/4fzgpIYcoIYcoIYcoIYcoIeeX88l/911TUxO3uT19V1dXm3tubq6v9/Te/f29uUejUXMfHx+P22KxmK/39JNxUkIOUUIOUUIOUULOpz/oeJGZmWnuc3Nz5l5RUWHuBQUFft2SaWdnJ25ze6tydXXV3B8eHny9p++EkxJyiBJyiBJyiBJyiBJypJ6+vcrOzjb3kpISc5+ZmTH34uJi3+7pvd3dXXOfnJw096WlJXN/fX317Z7UcVJCDlFCDlFCDlFCDlFCTqCfvr3Kyckx93A4bO6RSMTc8/Pz/bqlOPv7++Y+NjZm7svLy592L1+FkxJyiBJyiBJyiBJyiBJyftTTt1dFRUXmbj2Vd3V1ma91e+L36uXlxdzX19fNvbW11Zef+xU4KSGHKCGHKCGHKCGHKCGHp2+flJeXm3tPT4+5V1VVmXtzc7Onn3tycmLulZWV5h6ET7BzUkIOUUIOUUIOUUIOUUIOT99iHh8fzT0x0f4eLrcvrGppaTH3zc3ND93Xn8RJCTlECTlECTlECTlECTmf/tXKP10oFDL39vZ2c09ISPB0/e3tbXMPwlO2G05KyCFKyCFKyCFKyCFKyOHp2ydlZWXmPj09be5NTU2eru/2rWtu/40tyDgpIYcoIYcoIYcoIYcoIYen7w/o7OyM2xYWFszXpqWlebr28PCwuS8uLpr71dWVp+sHAScl5BAl5BAl5BAl5BAl5PB33/+gsLDQ3A8ODuK2m5sb87UbGxvm7vbNYrOzs+b+9vZzfk2clJBDlJBDlJBDlJDD24yO46SkpJj7/Py8uaempsZtvb295mvX1tY+fmM/FCcl5BAl5BAl5BAl5BAl5PD07TjO6OiouTc0NJj71tZW3Ob2FcfwjpMScogScogScogScogScr7l03d6erq5397emntGRoan61vviQfhK4uDgpMScogScogScogScogScgL9J7YdHR3m3tbWZu6Hh4fmHo1GPf3co6OjuK2+vt58bSwWM/fS0lJzHxwcNPf+/v7/dnPfACcl5BAl5BAl5BAl5BAl5ATi6TszM9Pcd3d3zb2goOAzb8fkdi9u//iqsbHR3J+enszd6/vzQcZJCTlECTlECTlECTlECTmB+OR5Xl6euWdlZf3hO3FXU1Pjy3USE+1fSV9fn7nf3d15ur7bl0FdX1+b+9nZmafr+4GTEnKIEnKIEnKIEnKIEnIC8d63G7en8qSkJHOvra0197q6OnMPhULm3t3d/e8357PLy0tz39vbM3fr658dx/2T8MfHx+Y+MjISt21ubpqv9QsnJeQQJeQQJeQQJeQQJeQE4r1vNxcXF55ef35+bu6Li4vmnpCQYO5+fAo8EomYe3JysrkXFRWZ+8DAgLm7fYI9HA6b++/fv83d+r/vPH3jxyFKyCFKyCFKyCFKyAn0e9/4njgpIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIYcoIedviqrpeT2dLWYAAAAASUVORK5CYII=\" id=\"imagea59d13834d\" transform=\"scale(1 -1) translate(0 -118.8)\" x=\"7.2\" y=\"-19.392052\" width=\"118.8\" height=\"118.8\"/>\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- 5 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(63.591026 14.0715) scale(0.096 -0.096)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-35\" d=\"M 266 1200 \n",
       "L 856 1250 \n",
       "Q 922 819 1161 601 \n",
       "Q 1400 384 1738 384 \n",
       "Q 2144 384 2425 690 \n",
       "Q 2706 997 2706 1503 \n",
       "Q 2706 1984 2436 2262 \n",
       "Q 2166 2541 1728 2541 \n",
       "Q 1456 2541 1237 2417 \n",
       "Q 1019 2294 894 2097 \n",
       "L 366 2166 \n",
       "L 809 4519 \n",
       "L 3088 4519 \n",
       "L 3088 3981 \n",
       "L 1259 3981 \n",
       "L 1013 2750 \n",
       "Q 1425 3038 1878 3038 \n",
       "Q 2478 3038 2890 2622 \n",
       "Q 3303 2206 3303 1553 \n",
       "Q 3303 931 2941 478 \n",
       "Q 2500 -78 1738 -78 \n",
       "Q 1113 -78 717 272 \n",
       "Q 322 622 266 1200 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-35\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g clip-path=\"url(#p99665436e2)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"imagea81a297d3f\" transform=\"scale(1 -1) translate(0 -118.8)\" x=\"149.1264\" y=\"-19.392052\" width=\"118.8\" height=\"118.8\"/>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- 0 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(205.517426 14.0715) scale(0.096 -0.096)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-30\" d=\"M 266 2259 \n",
       "Q 266 3072 433 3567 \n",
       "Q 600 4063 929 4331 \n",
       "Q 1259 4600 1759 4600 \n",
       "Q 2128 4600 2406 4451 \n",
       "Q 2684 4303 2865 4023 \n",
       "Q 3047 3744 3150 3342 \n",
       "Q 3253 2941 3253 2259 \n",
       "Q 3253 1453 3087 958 \n",
       "Q 2922 463 2592 192 \n",
       "Q 2263 -78 1759 -78 \n",
       "Q 1097 -78 719 397 \n",
       "Q 266 969 266 2259 \n",
       "z\n",
       "M 844 2259 \n",
       "Q 844 1131 1108 757 \n",
       "Q 1372 384 1759 384 \n",
       "Q 2147 384 2411 759 \n",
       "Q 2675 1134 2675 2259 \n",
       "Q 2675 3391 2411 3762 \n",
       "Q 2147 4134 1753 4134 \n",
       "Q 1366 4134 1134 3806 \n",
       "Q 844 3388 844 2259 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-30\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g clip-path=\"url(#pad7223a8ee)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAKUAAAClCAYAAAA9Kz3aAAAEi0lEQVR4nO3dvUscaxxH8VkJGgsFK8GkVBEELSSF1ga0TK2FiKhYWQRS+W8EYSGFdtrYJJ1gbFS08AV8KxRUEBsxFtrI7q3v9TeXmMzsnsXzKb8s2SkODzzsuikkSVJOJJC6aj+A9F9GKRyjFI5RCscohWOUwjFK4RilcIxSOEYpHKMUjlEKxyiFY5TCMUrhGKVwjFI4b6r9APq3sbGxcP/27Vu4F4vFcJ+amsrqkSrOk1I4RikcoxSOUQrHKIXj7RtmeHg43AuFQoWfpHo8KYVjlMIxSuEYpXC86NSIcvn1/A6ZJ6VwjFI4RikcoxSOUQrH23eN29jYqPYjZM6TUjhGKRyjFI5RCscohePtu0akfcm3VCpV+Eny50kpHKMUjlEKxyiFY5TC8fZdI9K+eX56elrhJ8mfJ6VwjFI4RikcoxSOUQrHKIVjlMIxSuEYpXCMUjhGKRw/+65xnz59CvfNzc0KP0l2PCmFY5TCMUrhGKVwjFI4hSRJcv0x7S9fvjzbBgYG8nzLVCsrK+F+dHQU7tW4wU5OTob7/Px8uJ+dnYV7e3t7Zs9UaZ6UwjFK4RilcIxSOEYpnMxu31+/fg33qampZ9vd3V342ouLiyweJenp6Qn3tF8ue3p6CveHh4dwPzw8fLZtbW2Fr93Z2Qn3nz9/hvvNzU24X11dhXtLS0u419fXh3st8KQUjlEKxyiFY5TCMUrhZPbN88bGxnCPbrzT09Pha5eXlzN5lt7e3t9+liRJksHBwXB///59uEc32/Hx8fC1ExMT4f74+Bjuabf4t2/fhntdXXyudHV1hfvx8XG4k3hSCscohWOUwjFK4RilcKryd9+3t7e5/vt7e3svev3u7u5fv+fMzEy4Nzc3h/vQ0FC4j46OhntDQ0O4p92++/r6wt3bt/QHjFI4RikcoxROVS46Hz58CPfV1dUKP0n+7u/vw31paelF++LiYriPjIyE+9zcXLh///493NO+eF0NnpTCMUrhGKVwjFI4RimczG7fJycnv/3a7u7urN721fj8+XO4p31BubOzM9ybmprC3du39D+MUjhGKRyjFI5RCiez2/ePHz/CPe0zWL1M2g9fnZ+fh3tra2u4v3v3LtwvLy//7MFy4EkpHKMUjlEKxyiFY5TCyez2vb+/H+5pt0Zl49evXy96/cePH8Od9F8xe1IKxyiFY5TCMUrhGKVwcv+771KplPdbvGppP1+d9ll2R0dHno+TCU9K4RilcIxSOEYpHKMUTu6374WFhWdbLdwAa8X19XW4F4vFcO/v7w930n8G5UkpHKMUjlEKxyiFY5TCyf32vba29mw7ODjI+21fjbTvFqT9PXhbW1u4l8vlzJ7pb3lSCscohWOUwjFK4RilcHK/fa+vr+f9Fgpsb2+H++zsbGUf5A94UgrHKIVjlMIxSuEYpXAKSZJwPvSUEk9KARmlcIxSOEYpHKMUjlEKxyiFY5TCMUrhGKVwjFI4RikcoxSOUQrHKIVjlMIxSuEYpXCMUjhGKRyjFI5RCscohWOUwjFK4RilcIxSOEYpnH8AbKfAxLJ/qAMAAAAASUVORK5CYII=\" id=\"image33b444904d\" transform=\"scale(1 -1) translate(0 -118.8)\" x=\"291.0528\" y=\"-19.392052\" width=\"118.8\" height=\"118.8\"/>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- 4 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(347.443826 14.0715) scale(0.096 -0.096)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0 \n",
       "L 2069 1097 \n",
       "L 81 1097 \n",
       "L 81 1613 \n",
       "L 2172 4581 \n",
       "L 2631 4581 \n",
       "L 2631 1613 \n",
       "L 3250 1613 \n",
       "L 3250 1097 \n",
       "L 2631 1097 \n",
       "L 2631 0 \n",
       "L 2069 0 \n",
       "z\n",
       "M 2069 1613 \n",
       "L 2069 3678 \n",
       "L 634 1613 \n",
       "L 2069 1613 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-34\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_4\">\n",
       "   <g clip-path=\"url(#p38b62da44b)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAKUAAAClCAYAAAA9Kz3aAAAEMUlEQVR4nO3dvy97exzH8VO5g8QgEokGERGDwWCWLsIisRE28WvwRzQRrDaDwaCxlMFgNhCTP4FB6IapEkMX7h3udvs+uY7y7bP1fIyvyMkZnj7JJ5XKJUnydyKBdDT7BaT/MkrhGKVwjFI4RikcoxSOUQrHKIVjlMIxSuEYpXCMUjhGKRyjFI5RCscohWOUwvmr2S/QiHw+H+6np6fhXigUwv3h4SHcR0dHv/ZiaognpXCMUjhGKRyjFI5RCqelb99jY2PhPjk5+YffRN/Jk1I4RikcoxSOUQrHKIVjlMIxSuEYpXCMUjhGKZyW/pixWq2G+9PTU7j39/eH+8DAQLjv7e2Fe7FYrNtqtVr4s8rOk1I4RikcoxSOUQrHKIWTS9rwnzsdHR2F+8rKSrh/fHxkev7IyEjdVqlUMj1D6TwphWOUwjFK4RilcIxSOG15+07z/v4e7llv38vLy3VbuVz+0jupnielcIxSOEYpHKMUjlEK51fdvl9eXsK9p6cn03Nubm7qttnZ2fBn397eMj1bnpQCMkrhGKVwjFI4RimcX3X7npiYCPezs7NwHxoa+vSzz8/Pw31hYeHTz9C/PCmFY5TCMUrhGKVwjFI4v+r2nebg4CDcNzY2Gn72+Ph4uN/d3TX87HblSSkcoxSOUQrHKIVjlMLx9p0kyfDwcLjf3983/Ozj4+NwX11dbfjZ7cqTUjhGKRyjFI5RCscohdPS/3Hsuzw+Pob7/Px8uJdKpbqtq6sr/Nno+9GTJEn6+vrC/fn5Odx/E09K4RilcIxSOEYpHKMUjp99f8Hm5mbdtr+/H/5sR0f8e39xcRHuS0tL4f76+vrJt2t9npTCMUrhGKVwjFI4XnS+IPqj4LQvyUr7Uq20fyi1u7sb7js7O596t3bgSSkcoxSOUQrHKIVjlMLx9v1Nent7wz3tj3bTbt/VajXcFxcXw/3y8vL/X67FeFIKxyiFY5TCMUrhGKVwvH3/sMPDw3BfW1vL9Jyrq6twn56ezvpKeJ6UwjFK4RilcIxSOEYpHG/fP6xQKIR7uVwO93w+n+n56+vr4Z72tdatwJNSOEYpHKMUjlEKxyiF4+27SWZmZsL95OQk3Lu7u8P99vY23Ofm5sI97S/ha7VauDeDJ6VwjFI4RikcoxSOUQrH2zfM1tZWuBeLxUzPSfta66mpqXC/vr7O9Pyf5EkpHKMUjlEKxyiFY5TC8V8rw1QqlXBP+2y6s7PzJ1+nKTwphWOUwjFK4RilcIxSON6+YUqlUrgPDg6G+/b2drinffady+W+9F5/kielcIxSOEYpHKMUjlEKx788F44npXCMUjhGKRyjFI5RCscohWOUwjFK4RilcIxSOEYpHKMUjlEKxyiFY5TCMUrhGKVwjFI4RikcoxSOUQrHKIVjlMIxSuEYpXCMUjhGKRyjFM4/24me6R5M5hIAAAAASUVORK5CYII=\" id=\"image582c2020f3\" transform=\"scale(1 -1) translate(0 -118.8)\" x=\"432.9792\" y=\"-19.392052\" width=\"118.8\" height=\"118.8\"/>\n",
       "   </g>\n",
       "   <g id=\"text_4\">\n",
       "    <!-- 1 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(489.370226 14.0715) scale(0.096 -0.096)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0 \n",
       "L 1822 0 \n",
       "L 1822 3584 \n",
       "Q 1619 3391 1289 3197 \n",
       "Q 959 3003 697 2906 \n",
       "L 697 3450 \n",
       "Q 1169 3672 1522 3987 \n",
       "Q 1875 4303 2022 4600 \n",
       "L 2384 4600 \n",
       "L 2384 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-31\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_5\">\n",
       "   <g clip-path=\"url(#p5bcb6f0242)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAKUAAAClCAYAAAA9Kz3aAAAE70lEQVR4nO3dsSv8fwDH8Q8JSZIYjMSCLNQtZBETiQFlwGZXSkJhs0iZDAZSJ0osWJCBfwAJZWCxqYsUvvvvXu9v37uuvO5+z8f4Svf99Ov5e9e74y4viqKfCDCS/9sPAPwXUcIOUcIOUcIOUcJOwW8/QK4rKyuT+9LSktwnJibk3tzcLPfr6+v0HswYJyXsECXsECXsECXsECXscPvOkNAtOx6Py72zs1Pue3t7cn96ekrvwbIQJyXsECXsECXsECXsECXscPvOkP7+frmHbtlnZ2dyn5+fl3sikUjrubIRJyXsECXsECXsECXsECXs5EX83XdG3N3dyb22tlbubW1tcr+8vMzYM2UrTkrYIUrYIUrYIUrY4W3GNDQ1NSVt9fX18me/v7/l/vDwkNFnyiWclLBDlLBDlLBDlLBDlLDD7TsN7e3tSVvolv3zw7u4qeKkhB2ihB2ihB2ihB2ihB1u32l4fHz87UfIaZyUsEOUsEOUsEOUsEOUsMOf2GbI19eX3EPvfVdXV8v99fU1Y8+UrTgpYYcoYYcoYYcoYYcoYYf3vtPQ3d3924+Q0zgpYYcoYYcoYYcoYYcoYYfbdxre39+Ttvx8/f936O/Bh4aG5L66upr+g+UITkrYIUrYIUrYIUrYIUrYyYrbdywWk/vh4aHcKyoq5H5/fy/3/f19uW9sbMhdfZFT6KuSQ1/iFPrSJ3BSwhBRwg5Rwg5Rwg5Rwo7V332Xl5fL/ebmRu5VVVVyz8vLk3umPn/85eXln//NVP++O/Tz/yeclLBDlLBDlLBDlLBDlLBj9d53YWGh3EO37JCamhq5f3x8yH1sbEzuvb29clff911aWip/NnTjr6yslHtPT4/cDw4O5J6LOClhhyhhhyhhhyhhhyhhJyve+769vZV76AYb+o30vr6+dB4rSV1dXdK2tbUlf7alpSWl197d3ZX7+Pi43BOJREqvnw04KWGHKGGHKGGHKGHH6qITMjs7K/e5uTm5f35+yv3o6Ejuw8PDclcfZBVSXFws966uLrlvb2+n9DqLi4tyD/03yGaclLBDlLBDlLBDlLBDlLCTFbfvkIWFBblPT0/LPfQLt2tra3KfmZmR+9vb2z883d+F/my4vr4+pdcZGBiQe+hDu7IBJyXsECXsECXsECXsECXsZPXtO2R0dFTu6+vrKb3OycmJ3AcHB5O2VG/kIyMjcg99pHXI8/Oz3FtbW+Ue+mAtJ5yUsEOUsEOUsEOUsEOUsJOTt++CAv25XaEb6fHxsdxLSkrkfn5+nrSF3oe/uLiQe1FRkdzj8bjcQ7/BHvpY66mpKbkvLy/L3QknJewQJewQJewQJewQJezk5O07Veq97CiKopWVFbmrD9YK3YJPT0/lHvrI7IaGBrmHhP7dnZ0duQ8NDaX0+r+BkxJ2iBJ2iBJ2iBJ2iBJ2uH3/RSwWk7v6rfGOjg75s42NjSn9m5OTk3JXX+ccRVHU1tYm983NTblfXV2l9Dy/gZMSdogSdogSdogSdogSdrh9ww4nJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJewQJez8AQxP3IaVxeZaAAAAAElFTkSuQmCC\" id=\"image9c6620d286\" transform=\"scale(1 -1) translate(0 -118.8)\" x=\"574.9056\" y=\"-19.392052\" width=\"118.8\" height=\"118.8\"/>\n",
       "   </g>\n",
       "   <g id=\"text_5\">\n",
       "    <!-- 9 -->\n",
       "    <g style=\"fill: #262626\" transform=\"translate(631.296626 14.0715) scale(0.096 -0.096)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-39\" d=\"M 350 1059 \n",
       "L 891 1109 \n",
       "Q 959 728 1153 556 \n",
       "Q 1347 384 1650 384 \n",
       "Q 1909 384 2104 503 \n",
       "Q 2300 622 2425 820 \n",
       "Q 2550 1019 2634 1356 \n",
       "Q 2719 1694 2719 2044 \n",
       "Q 2719 2081 2716 2156 \n",
       "Q 2547 1888 2255 1720 \n",
       "Q 1963 1553 1622 1553 \n",
       "Q 1053 1553 659 1965 \n",
       "Q 266 2378 266 3053 \n",
       "Q 266 3750 677 4175 \n",
       "Q 1088 4600 1706 4600 \n",
       "Q 2153 4600 2523 4359 \n",
       "Q 2894 4119 3086 3673 \n",
       "Q 3278 3228 3278 2384 \n",
       "Q 3278 1506 3087 986 \n",
       "Q 2897 466 2520 194 \n",
       "Q 2144 -78 1638 -78 \n",
       "Q 1100 -78 759 220 \n",
       "Q 419 519 350 1059 \n",
       "z\n",
       "M 2653 3081 \n",
       "Q 2653 3566 2395 3850 \n",
       "Q 2138 4134 1775 4134 \n",
       "Q 1400 4134 1122 3828 \n",
       "Q 844 3522 844 3034 \n",
       "Q 844 2597 1108 2323 \n",
       "Q 1372 2050 1759 2050 \n",
       "Q 2150 2050 2401 2323 \n",
       "Q 2653 2597 2653 3081 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-39\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p057c2b4809\">\n",
       "   <rect x=\"7.2\" y=\"20.0715\" width=\"118.120552\" height=\"118.120552\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p99665436e2\">\n",
       "   <rect x=\"149.1264\" y=\"20.0715\" width=\"118.120552\" height=\"118.120552\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pad7223a8ee\">\n",
       "   <rect x=\"291.0528\" y=\"20.0715\" width=\"118.120552\" height=\"118.120552\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p38b62da44b\">\n",
       "   <rect x=\"432.9792\" y=\"20.0715\" width=\"118.120552\" height=\"118.120552\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p5bcb6f0242\">\n",
       "   <rect x=\"574.9056\" y=\"20.0715\" width=\"118.120552\" height=\"118.120552\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize random digits\n",
    "images, labels = zip(*[train_dataset[i] for i in range(5)])\n",
    "show_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "import equinox as eqx\n",
    "from jaxtyping import Array, Float, Int, PyTree\n",
    "\n",
    "class CNN(eqx.Module):\n",
    "    \"\"\"Convolutional neural network for classifying handwritten digits from the MNIST dataset.\"\"\"\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jrandom.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(1, 32, kernel_size=3, key=key1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            eqx.nn.Conv2d(32, 64, kernel_size=3, key=key2),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(30976, 128, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(128, 10, key=key4),\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:  # Side note: These are shaped-array type hints, made possible by the package jaxtyping.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, here is the training code (modified from [this equinox tutorial](https://docs.kidger.site/equinox/examples/mnist/)), wrapped in the function `train` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import optax\n",
    "\n",
    "optim = optax.adamw(LEARNING_RATE)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(\n",
    "    model: CNN, \n",
    "    x: Float[Array, \"batch 1 28 28\"],  \n",
    "    y: Int[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_y = vmap(model)(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \"batch\"], \n",
    "    pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: CNN, \n",
    "    x: Float[Array, \"batch 1 28 28\"], \n",
    "    y: Int[Array, \"batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y = vmap(model)(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n",
    "def evaluate(model: CNN, testloader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)\n",
    "\n",
    "def train(\n",
    "    model: CNN,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    max_epochs: int,\n",
    "    max_steps: int,\n",
    "    print_every: int,\n",
    ") -> CNN:\n",
    "    \"\"\"Trains the CNN.\"\"\"\n",
    "\n",
    "    # Initialise the optimizer state.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: CNN,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 1 28 28\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Training loop\n",
    "    step = 0\n",
    "    max_steps_reached = False\n",
    "    start_time = time()\n",
    "    for epoch in range(max_epochs):\n",
    "        for x, y in trainloader:\n",
    "            x = x.numpy()\n",
    "            y = y.numpy()\n",
    "            model = eqx.nn.inference_mode(model, value=False)\n",
    "            model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "            if (step % print_every) == 0:\n",
    "                model = eqx.nn.inference_mode(model, value=True)\n",
    "                test_loss, test_accuracy = evaluate(model, testloader)\n",
    "                hr, min, sec = convert_seconds_to_hms(time() - start_time)\n",
    "                print(\n",
    "                    f\"elapsed_time = {hr:1d}h {min:1d}m {sec:.1f}s, {epoch=}, {step=}, train_loss={train_loss.item():.2f}, \"\n",
    "                    f\"test_loss={test_loss.item():.2f}, test_accuracy={test_accuracy.item():.3f}\"\n",
    "                )\n",
    "            step += 1\n",
    "            if step >= max_steps:\n",
    "                max_steps_reached = True\n",
    "                print(\"Training complete. (Maximum number of steps reached.)\")\n",
    "                break\n",
    "        if max_steps_reached:\n",
    "            break\n",
    "    if not max_steps_reached:\n",
    "        print(\"Training complete.\")\n",
    "    hr, min, sec = convert_seconds_to_hms(time() - start_time)\n",
    "    print(f\"Total elapsed_time = {hr:1d}h {min:1d}m {sec:.1f}s\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's talk a bit about JAX devices.\n",
    "\n",
    "JAX will automatically detect all available devices (e.g., CPU, GPU) to use for computation. Let's look at what devices are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU devices:  [CpuDevice(id=0)]\n",
      "GPU devices:  [cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"CPU devices: \", jax.devices('cpu'))\n",
    "try:\n",
    "    is_gpu_avail = True\n",
    "    print(\"GPU devices: \", jax.devices('gpu'))\n",
    "except RuntimeError as e:\n",
    "    is_gpu_avail = False\n",
    "    print(\"GPU devices: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a GPU device was detected, then JAX will automatically make GPU the default backend. Otherwise, CPU is the default backend. Unless otherwise specified, *JAX will do all computations in the default backend*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default backend is: gpu\n"
     ]
    }
   ],
   "source": [
    "print('The default backend is:', jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, ***JAX will automatically use the GPU if available*** (provided that you have installed the correct version of JAX compatible with your GPU). This is very convenient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committing arrays to a device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may want to manually \"force\" computations to happen on devices/backends other than the default one. You do this by ***committing*** JAX arrays to a device. Then, all subsequent operations on that array will happen on the *committed* device. This is accomplished using the function [`jax.device_put`](https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html#jax.device_put). See the [JAX FAQ](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices) for more details. \n",
    "\n",
    "Let's demonstrate this. First, we initialize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(key=key)  # An instance of our CNN class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the arrays in the pytree `model` are uncommitted, so subsequent operations on `model` will happen on the default device. Since our goal is to compare CPU and GPU runtimes, we will make two copies of `model`&mdash;one on CPU and the other on GPU.\n",
    "\n",
    "Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for committing JAX arrays to a GPU or CPU\n",
    "put_on_cpu = lambda x: jax.device_put(x, jax.devices('cpu')[0]) if isinstance(x, jax.Array) else x\n",
    "put_on_gpu = lambda x: jax.device_put(x, jax.devices('gpu')[0]) if isinstance(x, jax.Array) else x\n",
    "\n",
    "key, key_gpu, key_cpu = jrandom.split(key, 3)\n",
    "model_cpu = jax.tree.map(put_on_cpu, CNN(key=key_cpu))\n",
    "if is_gpu_avail:\n",
    "    model_gpu = jax.tree.map(put_on_gpu, CNN(key=key_gpu))\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! All arrays in `model_cpu` are committed to CPU, and all arrays in `model_gpu` are committed to GPU. Let's check one of the arrays just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CPU's copy of model is on:  {CpuDevice(id=0)}\n",
      "The GPU's copy of model is on:  {cuda(id=0)}\n"
     ]
    }
   ],
   "source": [
    "print('The CPU\\'s copy of model is on: ', model_cpu.layers[0].weight.devices())\n",
    "if is_gpu_avail:\n",
    "    print('The GPU\\'s copy of model is on: ', model_gpu.layers[0].weight.devices())\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. We are now ready to benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU vs. GPU runtime\n",
    "\n",
    "Let's start by training on CPU (this may take a few minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training on CPU.\n",
      "elapsed_time = 0h 0m 32.0s, epoch=0, step=0, train_loss=2.31, test_loss=2.28, test_accuracy=0.193\n",
      "elapsed_time = 0h 1m 3.2s, epoch=0, step=10, train_loss=1.11, test_loss=1.04, test_accuracy=0.721\n",
      "elapsed_time = 0h 1m 37.8s, epoch=0, step=20, train_loss=0.55, test_loss=0.58, test_accuracy=0.815\n",
      "elapsed_time = 0h 2m 7.9s, epoch=0, step=30, train_loss=0.40, test_loss=0.43, test_accuracy=0.862\n",
      "elapsed_time = 0h 2m 49.0s, epoch=0, step=40, train_loss=0.44, test_loss=0.38, test_accuracy=0.884\n",
      "elapsed_time = 0h 3m 19.7s, epoch=0, step=50, train_loss=0.46, test_loss=0.32, test_accuracy=0.909\n",
      "elapsed_time = 0h 3m 52.6s, epoch=0, step=60, train_loss=0.24, test_loss=0.27, test_accuracy=0.922\n",
      "elapsed_time = 0h 4m 22.7s, epoch=0, step=70, train_loss=0.14, test_loss=0.27, test_accuracy=0.919\n",
      "elapsed_time = 0h 4m 53.1s, epoch=0, step=80, train_loss=0.28, test_loss=0.23, test_accuracy=0.935\n",
      "elapsed_time = 0h 5m 23.1s, epoch=0, step=90, train_loss=0.38, test_loss=0.22, test_accuracy=0.939\n",
      "Training complete. (Maximum number of steps reached.)\n",
      "Total elapsed_time = 0h 5m 28.7s"
     ]
    }
   ],
   "source": [
    "print(\"Begin training on CPU.\")\n",
    "trained_model_cpu = train(model_cpu, trainloader, testloader, optim, MAX_EPOCHS, MAX_STEPS, PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly slow. Let's try training on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training on GPU.\n",
      "elapsed_time = 0h 0m 3.6s, epoch=0, step=0, train_loss=2.31, test_loss=2.30, test_accuracy=0.195\n",
      "elapsed_time = 0h 0m 6.9s, epoch=0, step=10, train_loss=1.21, test_loss=1.13, test_accuracy=0.698\n",
      "elapsed_time = 0h 0m 10.7s, epoch=0, step=20, train_loss=0.54, test_loss=0.54, test_accuracy=0.829\n",
      "elapsed_time = 0h 0m 13.5s, epoch=0, step=30, train_loss=0.55, test_loss=0.41, test_accuracy=0.868\n",
      "elapsed_time = 0h 0m 16.3s, epoch=0, step=40, train_loss=0.38, test_loss=0.40, test_accuracy=0.871\n",
      "elapsed_time = 0h 0m 19.1s, epoch=0, step=50, train_loss=0.22, test_loss=0.34, test_accuracy=0.897\n",
      "elapsed_time = 0h 0m 22.8s, epoch=0, step=60, train_loss=0.44, test_loss=0.28, test_accuracy=0.920\n",
      "elapsed_time = 0h 0m 25.7s, epoch=0, step=70, train_loss=0.27, test_loss=0.27, test_accuracy=0.924\n",
      "elapsed_time = 0h 0m 28.8s, epoch=0, step=80, train_loss=0.33, test_loss=0.23, test_accuracy=0.937\n",
      "elapsed_time = 0h 0m 31.6s, epoch=0, step=90, train_loss=0.26, test_loss=0.22, test_accuracy=0.938\n",
      "Training complete. (Maximum number of steps reached.)\n",
      "Total elapsed_time = 0h 0m 31.7s"
     ]
    }
   ],
   "source": [
    "print(\"Begin training on GPU.\")\n",
    "trained_model_gpu = train(model_gpu, trainloader, testloader, optim, MAX_EPOCHS, MAX_STEPS, PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much faster! Using the GPU resulted in 5-10X speedup over CPU for this small example. For larger models, the speedup will be even more dramatic. This is why virtually all modern deep learning models are trained on GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
